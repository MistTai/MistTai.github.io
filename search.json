[{"title":"缓存处理","url":"/2021/12/29/缓存处理/","content":"在Java开发项目中，为了提高查询的性能，一般采用Redis缓存解决。\n<!--more-->\n##  Redis环境搭建\n以docker的形式搭建Redis服务\n```\ndocker run -di --name=tensquare_redis -p 6379:6379 redis\n```\n## SpringDataRedis\nSpring-data-redis是spring大家族的一部分，提供了在srping应用中通过简单的配置访问\nredis服务，对reids底层开发包(Jedis, JRedis, and RJC)进行了高度封装，RedisTemplate\n提供了redis各种操作。\n1. 先通过pom.xml引入依赖\n\t```\n    <dependency>\n    \t<groupId>org.springframework.boot</groupId>\n    \t<artifactId>spring-boot-starter-data-redis</artifactId>\n    </dependency>\n\n   ```\n2. 修改application.yml，在spring节点下添加配置\n```\nredis:\n\thost: 192.168.184.134\n```\n3. 引入RedisTemplate,并加入缓存\n\n  ```\n      @Autowired\n      private RedisTemplate redisTemplate;\n    /**\n    * 根据ID查询实体\n    * @param id\n    * @return\n    */\n    public Article findById(String id) {\n      //从缓存中提取\n      Article article=\n      (Article)redisTemplate.opsForValue().get(\"article_\"+id);\n      // 如果缓存没有则到数据库查询并放入缓存\n      if(article==null) {\n      article = articleDao.findById(id).get();\n      redisTemplate.opsForValue().set(\"article_\" + id, article);\n      }\n      return article;\n    }\n\n  ```\n4. 修改或删除后清除缓存\n\n  ```\n    /**\n    * 修改\n    * @param article\n    */\n    public void update(Article article) {\n      redisTemplate.delete( \"article_\" + article.getId() );//删除缓存\n      articleDao.save(article);\n    }\n    /**\n    * 删除\n    * @param id\n    */\n    public void deleteById(String id) {\n      redisTemplate.delete( \"article_\" + id );//删除缓存\n      articleDao.deleteById(id);\n    }\n\n  ```\n5. 缓存过期处理\n\n设置一天过期时间\n  ```\n  redisTemplate.opsForValue().set(\"article_\" + id, article,1,\n  TimeUnit.DAYS);\n  ```\n  \n## Spring Cache\n\nSpring Cache使用方法与Spring对事务管理的配置相似。Spring Cache的核心就是对某\n个方法进行缓存，其实质就是缓存该方法的返回结果，并把方法参数和结果用键值对的\n方式存放到缓存中，当再次调用该方法使用相应的参数时，就会直接从缓存里面取出指\n定的结果进行返回。\n\n@Cacheable-------使用这个注解的方法在执行后会缓存其返回结果。\n@CacheEvict--------使用这个注解的方法在其执行前或执行后移除Spring Cache中的某些\n元素。\n\n参考这篇博客\nhttps://www.cnblogs.com/fashflying/p/6908028.html\n","tags":["Spring Cloud","缓存"],"categories":["Java"]},{"title":"分布式ID生成器","url":"/2021/12/27/分布式ID生成器/","content":"由于我们的数据库在生产环境中要分片部署（MyCat）,所以我们不能使用数据库本\n身的自增功能来产生主键值，只能由程序来生成唯一的主键值。可以采用开源的\ntwitter( 非官方中文惯称：推特.是国外的一个网站，是一个社交网络及微博客服务) 的\nsnowflake （雪花）算法。\n<!--more-->\n\n![玖涯博客](/2021/12/27/分布式ID生成/pasted-0.png)\n默认情况下41bit的时间戳可以支持该算法使用到2082年，10bit的工作机器id可以\n支持1024台机器，序列号支持1毫秒产生4096个自增序列id . SnowFlake的优点是，整\n体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID\n作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。\n\n```\npackage util;\n\nimport java.lang.management.ManagementFactory;\nimport java.net.InetAddress;\nimport java.net.NetworkInterface;\n\n/**\n * <p>名称：IdWorker.java</p>\n * <p>描述：分布式自增长ID</p>\n * <pre>\n *     Twitter的 Snowflake　JAVA实现方案\n * </pre>\n * 核心代码为其IdWorker这个类实现，其原理结构如下，我分别用一个0表示一位，用—分割开部分的作用：\n * 1||0---0000000000 0000000000 0000000000 0000000000 0 --- 00000 ---00000 ---000000000000\n * 在上面的字符串中，第一位为未使用（实际上也可作为long的符号位），接下来的41位为毫秒级时间，\n * 然后5位datacenter标识位，5位机器ID（并不算标识符，实际是为线程标识），\n * 然后12位该毫秒内的当前毫秒内的计数，加起来刚好64位，为一个Long型。\n * 这样的好处是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和机器ID作区分），\n * 并且效率较高，经测试，snowflake每秒能够产生26万ID左右，完全满足需要。\n * <p>\n * 64位ID (42(毫秒)+5(机器ID)+5(业务编码)+12(重复累加))\n *\n * @author Polim\n */\npublic class IdWorker {\n    // 时间起始标记点，作为基准，一般取系统的最近时间（一旦确定不能变动）\n    private final static long twepoch = 1288834974657L;\n    // 机器标识位数\n    private final static long workerIdBits = 5L;\n    // 数据中心标识位数\n    private final static long datacenterIdBits = 5L;\n    // 机器ID最大值\n    private final static long maxWorkerId = -1L ^ (-1L << workerIdBits);\n    // 数据中心ID最大值\n    private final static long maxDatacenterId = -1L ^ (-1L << datacenterIdBits);\n    // 毫秒内自增位\n    private final static long sequenceBits = 12L;\n    // 机器ID偏左移12位\n    private final static long workerIdShift = sequenceBits;\n    // 数据中心ID左移17位\n    private final static long datacenterIdShift = sequenceBits + workerIdBits;\n    // 时间毫秒左移22位\n    private final static long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;\n\n    private final static long sequenceMask = -1L ^ (-1L << sequenceBits);\n    /* 上次生产id时间戳 */\n    private static long lastTimestamp = -1L;\n    // 0，并发控制\n    private long sequence = 0L;\n\n    private final long workerId;\n    // 数据标识id部分\n    private final long datacenterId;\n\n    public IdWorker() {\n        this.datacenterId = getDatacenterId(maxDatacenterId);\n        this.workerId = getMaxWorkerId(datacenterId, maxWorkerId);\n    }\n\n    /**\n     * @param workerId     工作机器ID\n     * @param datacenterId 序列号\n     */\n    public IdWorker(long workerId, long datacenterId) {\n        if (workerId > maxWorkerId || workerId < 0) {\n            throw new IllegalArgumentException(String.format(\"worker Id can't be greater than %d or less than 0\", maxWorkerId));\n        }\n        if (datacenterId > maxDatacenterId || datacenterId < 0) {\n            throw new IllegalArgumentException(String.format(\"datacenter Id can't be greater than %d or less than 0\", maxDatacenterId));\n        }\n        this.workerId = workerId;\n        this.datacenterId = datacenterId;\n    }\n\n    /**\n     * 获取下一个ID\n     *\n     * @return\n     */\n    public synchronized long nextId() {\n        long timestamp = timeGen();\n        if (timestamp < lastTimestamp) {\n            throw new RuntimeException(String.format(\"Clock moved backwards.  Refusing to generate id for %d milliseconds\", lastTimestamp - timestamp));\n        }\n\n        if (lastTimestamp == timestamp) {\n            // 当前毫秒内，则+1\n            sequence = (sequence + 1) & sequenceMask;\n            if (sequence == 0) {\n                // 当前毫秒内计数满了，则等待下一秒\n                timestamp = tilNextMillis(lastTimestamp);\n            }\n        } else {\n            sequence = 0L;\n        }\n        lastTimestamp = timestamp;\n        // ID偏移组合生成最终的ID，并返回ID\n        long nextId = ((timestamp - twepoch) << timestampLeftShift)\n                | (datacenterId << datacenterIdShift)\n                | (workerId << workerIdShift) | sequence;\n\n        return nextId;\n    }\n\n    private long tilNextMillis(final long lastTimestamp) {\n        long timestamp = this.timeGen();\n        while (timestamp <= lastTimestamp) {\n            timestamp = this.timeGen();\n        }\n        return timestamp;\n    }\n\n    private long timeGen() {\n        return System.currentTimeMillis();\n    }\n\n    /**\n     * <p>\n     * 获取 maxWorkerId\n     * </p>\n     */\n    protected static long getMaxWorkerId(long datacenterId, long maxWorkerId) {\n        StringBuffer mpid = new StringBuffer();\n        mpid.append(datacenterId);\n        String name = ManagementFactory.getRuntimeMXBean().getName();\n        if (!name.isEmpty()) {\n            /*\n             * GET jvmPid\n             */\n            mpid.append(name.split(\"@\")[0]);\n        }\n        /*\n         * MAC + PID 的 hashcode 获取16个低位\n         */\n        return (mpid.toString().hashCode() & 0xffff) % (maxWorkerId + 1);\n    }\n\n    /**\n     * <p>\n     * 数据标识id部分\n     * </p>\n     */\n    protected static long getDatacenterId(long maxDatacenterId) {\n        long id = 0L;\n        try {\n            InetAddress ip = InetAddress.getLocalHost();\n            NetworkInterface network = NetworkInterface.getByInetAddress(ip);\n            if (network == null) {\n                id = 1L;\n            } else {\n                byte[] mac = network.getHardwareAddress();\n                id = ((0x000000FF & (long) mac[mac.length - 1])\n                        | (0x0000FF00 & (((long) mac[mac.length - 2]) << 8))) >> 6;\n                id = id % (maxDatacenterId + 1);\n            }\n        } catch (Exception e) {\n            System.out.println(\" getDatacenterId: \" + e.getMessage());\n        }\n        return id;\n    }\n}\n\n```\n在启动类中注册bean\n\n```\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\nimport org.springframework.context.annotation.Bean;\nimport util.IdWorker;\n\n@SpringBootApplication\n//@CrossOrigin //允许跨域\n@EnableEurekaClient\npublic class BaseApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(BaseApplication.class);\n    }\n\n    @Bean\n    public IdWorker idWorker(){\n        return new IdWorker(1, 1);\n    }\n}\n```","tags":["Java"],"categories":["Java"]},{"title":"Spring Cloud","url":"/2021/12/20/Spring-Cloud/","content":"# Spring Cloud简介\n## 什么是Spring Cloud\nSpring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简\n化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、\n熔断器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并\n没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框\n架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给\n开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。\n\n<!--more-->\n\n## Spring Cloud和Spring Boot的关系\nSpring Boot 是 Spring 的一套快速配置脚手架，可以基于Spring Boot 快速开发单\n个微服务，Spring Cloud是一个基于Spring Boot实现的云应用开发工具；Spring Boot专\n注于快速、方便集成的单个微服务个体，Spring Cloud关注全局的服务治理框架；\nSpring Boot使用了默认大于配置的理念，很多集成方案已经帮你选择好了，能不配置就\n不配置，Spring Cloud很大的一部分是基于Spring Boot来实现，可以不基于Spring Boot\n吗？不可以。\n\nSpring Boot可以离开Spring Cloud独立使用开发项目，但是Spring Cloud离不开\nSpring Boot，属于依赖的关系。\n\n## Spring Cloud主要框架\n1. 服务发现--Netflix Eureka\n2. 服务调用--Netflix Feign\n3. 熔断器--Netflix Hystrix\n4. 服务网关--Netflix Zuul\n5. 分布式配置--Spring Cloud Config\n6. 消息总线--Spring Cloud Bus\n\n# 服务发现组件 Eureka\n## Eureka\nEureka是Netflix开发的服务发现框架，SpringCloud将它集成在自己的子项目\nspring-cloud-netflix中，实现SpringCloud的服务发现功能。Eureka包含两个组件：\nEureka Server和Eureka Client。\n\nEureka Server提供服务注册服务，各个节点启动后，会在Eureka Server中进行注\n册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点\n的信息可以在界面中直观的看到。\n\nEureka Client是一个java客户端，用于简化与Eureka Server的交互，客户端同时也\n就别一个内置的、使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会\n向Eureka Server发送心跳,默认周期为30秒，如果Eureka Server在多个心跳周期内没有\n接收到某个节点的心跳，Eureka Server将会从服务注册表中把这个服务节点移除(默认90\n秒)。\n\nEureka Server之间通过复制的方式完成数据的同步，Eureka还提供了客户端缓存机\n制，即使所有的Eureka Server都挂掉，客户端依然可以利用缓存中的信息消费其他服务\n的API。综上，Eureka通过心跳检查、客户端缓存等机制，确保了系统的高可用性、灵活\n性和可伸缩性。\n\n## Eureka服务端开发\n（1）创建hbgydx_eureka模块\n\n（2）引入依赖 父工程pom.xml定义Spring Cloud版本\n```\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-dependencies</artifactId>\n    <version>Finchley.M9</version>\n    <type>pom</type>\n    <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n```\n\nhbgydx_eureka模块pom.xml引入eureka-server\n```\n<dependencies>\n  <dependency>\n  \t<groupId>org.springframework.cloud</groupId>\n  \t<artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n  </dependency>\n</dependencies>\n\n```\n(3)添加application.yml\n```\nserver:\n\tport: 6868 #服务端口\neureka:\n\tclient:\n\t\tregisterWithEureka: false #是否将自己注册到Eureka服务中，本身就是所yi无需\n注册\n\t\tfetchRegistry: false #是否从Eureka中获取注册信息\n\t\tserviceUrl: #Eureka客户端与Eureka服务端进行交互的地址\n\t\t\tdefaultZone: http://127.0.0.1:${server.port}/eureka/\n```\n（4）编写启动类\n```\n@SpringBootApplication\n@EnableEurekaServer\npublic class EurekaServer {\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(EurekaServer.class, args);\n\t}\n}\n\n```\n(5)启动运行启动类，然后在浏览器地址栏输入 http://localhost:6868/ 运行.主界面中system status为系统信息 General Info为一般信息 Instances currently registered with Eureka为注册到的所有微服务列表\n\n## 服务注册\n我们现在就将所有的微服务都注册到Eureka中，这样所有的微服务之间都可以互相调用\n了。\n（1）将其他微服务模块添加依赖\n```\n<dependency>\n\t<groupId>org.springframework.cloud</groupId>\n\t<artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n</dependency>\n```\n(2)修改每个微服务的application.yml,添加注册eureka服务配置\n```\neureka:\n\tclient:\n\t\tservice-url:\n\t\t\tdefaultZone: http://localhost:6868/eureka\n\tinstance:\n\t\tprefer-ip-address: true\n```\n(3)修改每个服务i类的启动类，添加注解：@EnableEurekaClient\n\n(4)启动测试：将每个微服务启动起来，会发现eureka的注册列表中可以看到这些微服\n务了\n\n# Feign实现服务间的调用\n## Feign简介\nFeign是简化Java HTTP客户端开发的工具（java-to-httpclient-binder），它的灵感\n来自于Retrofit、JAXRS-2.0和WebSocket。Feign的初衷是降低统一绑定Denominator到\nHTTP API的复杂度，不区分是否为restful。\n## 快速体验\n（1）在hbgydx_qa模块添加依赖\n```\n<dependency>\n\t<groupId>org.springframework.cloud</groupId>\n\t<artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n\n```\n(2)修改hbgydx_qa模块的启动类，添加注解\n```\n@EnableDiscoveryClient\n@EnableFeignClients\n\n```\n(3)在hbgydx_qa模块创建com.hbgydx.qa.client包，包下创建接口：\n```\n@FeignClient(\"base\")\npublic interface LabelClient {\n\t@RequestMapping(value=\"/label/{id}\", method = RequestMethod.GET)\n\tpublic Result findById(@PathVariable(\"id\") String id);\n}\n\n```\n@FeignClint注解用于指定从哪个服务中调用功能，注意，里面的名称与调用的服务名保持一致，并且不能有下划线\n@RequestMapping注解用于对被调用的微服务进行地址映射。注意，@PathVariable注解一定要指定参数名称，否则出错\n（5）修改hbgydx_qa模块的ProblemController\n```\n@Autowired\nprivate LabelClient labelClient;\n@RequestMapping(value = \"/label/{labelid}\")\npublic Result findLabelById(@PathVariable String labelid){\n\tResult result = labelClient.findById(labelid);\n\treturn result;\n}\n\n```","tags":["Spring Cloud","Java"],"categories":["Java"]},{"title":"Dockerfile","url":"/2021/12/15/Dockerfile/","content":"## 什么是Dockerfile\nDockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。\n1. 对于开发人员：可以为开发团队提供一个完全一致的开发环境\n2. 对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了。\n3. 对于运维人员：在部署时，可以实现应用的无缝移植\n<!--more-->\n\n## 常用命令\n1. FROM image_name:tag 定义了使用哪个基础镜像启动构建流程\n2. MAINTAINER user_name  声明镜像的创建者\n3. ENV key value 设置环境变量 (可以写多条)\n4. RUN command 是Dockerfile的核心部分(可以写多条)\n5. ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压.\n6. ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压\n7. WORKDIR path_dir 设置工作目录\n8. EXPOSE port1 prot2 用来指定端口，使容器内的应用可以通过端口和外界交互\n9. CMD argument 在构建容器时使用，会被docker run 后的argument覆盖\n10. ENTRYPOINT argument 和CMD相似，但是并不会被docker run指定的参数覆盖\n11. VOLUME 将本地文件夹或者其他容器的文件挂载到容器中\n\n## 使用脚本创建镜像\n1. 创建目录\n\n  ```\n  mkdir –p /usr/local/dockerjdk8\n  ```\n\n2. 下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中\n的/usr/local/dockerjdk8目录\n3. 创建文件Dockerfile vi Dockerfile\n\t```\n    #依赖镜像名称和ID\nFROM centos:7\n#指定镜像创建者信息\nMAINTAINER ITCAST\n#切换工作目录\nWORKDIR /usr\nRUN mkdir /usr/local/java\n#ADD 是相对路径jar,把java添加到容器中\nADD jdk‐8u171‐linux‐x64.tar.gz /usr/local/java/\n#配置java环境变量\nENV JAVA_HOME /usr/local/java/jdk1.8.0_171\nENV JRE_HOME $JAVA_HOME/jre\nENV CLASSPATH\n$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH\nENV PATH $JAVA_HOME/bin:$PATH\n   ```\n4. 执行命令构建镜像\n\t```\n    docker build ‐t='jdk1.8' .\n   ```\n5.查看镜像是否建立完成\n\t```\n    docker images\n\n   ```\n6. 创建容器\n\t```\n    docker run ‐it ‐‐name=myjdk8 jdk1.8 /bin/bash\n   ```","tags":["java","Spring Cloud"],"categories":["Java"]},{"title":"JJWT","url":"/2021/09/11/JJWT/","content":"JJWT是一个提供端到端的JWT创建和验证的Java库。永远免费和开源(Apache\nLicense，版本2.0)，JJWT很容易使用和理解。它被设计成一个以建筑为中心的流畅界\n面，隐藏了它的大部分复杂性。\n<!--more-->\n# JJWT快速入门\n## token的创建\n1. 创建maven工程，引入依赖\n\n\t```\n    <dependency>\n        <groupId>io.jsonwebtoken</groupId>\n        <artifactId>jjwt</artifactId>\n        <version>0.6.0</version>\n    </dependency\n\t```\n\n2. 创建类CreateJwtTest，用于生成token\n\t```\n    public class CreateJwtTest {\n      public static void main(String[] args) {\n      JwtBuilder builder= Jwts.builder().setId(\"888\")\n      .setSubject(\"小白\")\n      .setIssuedAt(new Date())\n      .signWith(SignatureAlgorithm.HS256,\"hbgydx\");\n      System.out.println( builder.compact() );\n      }\n     }\n\n   ```\n\tsetIssuedAt用于设置签发时间\n\n\tsignWith用于设置签名秘钥 \n    \n3. 测试运行\n\n## token的解析\n\t\n   我们刚才已经创建了token ，在web应用中这个操作是由服务端进行然后发给客户\n端，客户端在下次向服务端发送请求时需要携带这个token（这就好像是拿着一张门票一\n样），那服务端接到这个token 应该解析出token中的信息（例如用户id）,根据这些信息\n查询数据库返回相应的结果。\n\n创建ParseJwtTest\n```\npublic class ParseJwtTest {\n  public static void main(String[] args) {\n  String\n  token=\"eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiO\n  jE1MjM0MTM0NTh9.gq0J-cOM_qCNqU_s-d_IrRytaNenesPmqAIhQpYXHZk\";\n  Claims claims =\n  Jwts.parser().setSigningKey(\"hbgydx\").parseClaimsJws(token).getBody();\n  System.out.println(\"id:\"+claims.getId());\n  System.out.println(\"subject:\"+claims.getSubject());\n  System.out.println(\"IssuedAt:\"+claims.getIssuedAt());\n  }\n}\n```\n试着将token或签名秘钥篡改一下，会发现运行时就会报错，所以解析token也就是验证\ntoken\n\n## token过期校验\n有很多时候，我们并不希望签发的token是永久生效的，所以我们可以为token添加一个\n过期时间。\n\n创建CreateJwtTest2\n\n```\npublic class CreateJwtTest2 {\n  public static void main(String[] args) {\n    //为了方便测试，我们将过期时间设置为1分钟\n    long now = System.currentTimeMillis();//当前时间\n    long exp = now + 1000*60;//过期时间为1分钟\n    JwtBuilder builder= Jwts.builder().setId(\"888\")\n    .setSubject(\"小白\")\n    .setIssuedAt(new Date())\n    .signWith(SignatureAlgorithm.HS256,\"hbgydx\")\n    .setExpiration(new Date(exp));\n    System.out.println( builder.compact() );\n  }\n}\n```\n\nsetExpiration 方法用于设置过期时间\n\n当未过期时可以正常读取，当过期时会引发io.jsonwebtoken.ExpiredJwtException异常。\n\n## 自定义claims\n刚才的例子只是存储了id和subject两个信息，如果你想存储更多的信息（例如角\n色）可以定义自定义claims 创建CreateJwtTest3\n\n```\npublic class CreateJwtTest3 {\n  public static void main(String[] args) {\n    //为了方便测试，我们将过期时间设置为1分钟\n    long now = System.currentTimeMillis();//当前时间\n    long exp = now + 1000*60;//过期时间为1分钟\n    JwtBuilder builder= Jwts.builder().setId(\"888\")\n    .setSubject(\"小白\")\n    .setIssuedAt(new Date())\n    .signWith(SignatureAlgorithm.HS256,\"hbgydx\")\n    .setExpiration(new Date(exp))\n    .claim(\"roles\",\"admin\")\n    .claim(\"logo\",\"logo.png\");\n    System.out.println( builder.compact() );\n  }\n}\n\n```\n\n修改ParseJwtTest\n\n```\npublic class ParseJwtTest {\n  public static void main(String[] args) {\n    String\n    compactJws=\"eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJp\n    YXQiOjE1MjM0MTczMjMsImV4cCI6MTUyMzQxNzM4Mywicm9sZXMiOiJhZG1pbiIsImxvZ28iO\n    iJsb2dvLnBuZyJ9.b11p4g4rE94rqFhcfzdJTPCORikqP_1zJ1MP8KihYTQ\";\n    Claims claims =\n    Jwts.parser().setSigningKey(\"hbgydx\").parseClaimsJws(compactJws).getBody(\n    );\n    System.out.println(\"id:\"+claims.getId());\n    System.out.println(\"subject:\"+claims.getSubject());\n    System.out.println(\"roles:\"+claims.get(\"roles\"));\n    System.out.println(\"logo:\"+claims.get(\"logo\"));\n    SimpleDateFormat sdf=new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\");\n    System.out.println(\"签发时间:\"+sdf.format(claims.getIssuedAt()));\n    System.out.println(\"过期时\n    间:\"+sdf.format(claims.getExpiration()));\n    System.out.println(\"当前时间:\"+sdf.format(new Date()) );\n  }\n}\n```","tags":["JJWT","jwt"],"categories":["Java"]},{"title":"IOU & GIOU & DIOU 介绍及其代码实现","url":"/2021/09/02/iou/","content":"参考这篇博客\nhttps://blog.csdn.net/leonardohaig/article/details/103394369","tags":["iou"]},{"title":"基于JWT的Token认证机制实现","url":"/2021/09/01/基于JWT的Token认证机制实现/","content":"# 什么是JWT\nJSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在用\n户和服务器之间传递安全可靠的信息。\n<!--more-->\n# JWT组成\n一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名。\n\n**头部（Header）**\n\n头部用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。这也可以\n被表示成一个JSON对象。\n\n\t{\"typ\":\"JWT\",\"alg\":\"HS256\"}\n\n在头部指明了签名算法是HS256算法。 我们进行BASE64编\n码http://base64.xpcha.com/，编码后的字符串如下：\n\n\teyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9\n\n\t小知识：Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于2\n\t的6次方等于64，所以每6个比特为一个单元，对应某个可打印字符。三个字节有24\n\t个比特，对应于4个Base64单元，即3个字节需要用4个可打印字符来表示。JDK 中\n\t提供了非常方便的 BASE64Encoder 和 BASE64Decoder，用它们可以非常方便的\n\t完成基于 BASE64 的编码和解码\n**载荷（playload）**\n\n载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包\n含三个部分\n1. 标准中注册的声明（建议但不强制使用）\n  \t\n   \t   iss: jwt签发者\n  \t\tsub: jwt所面向的用户\n  \t\taud: 接收jwt的一方\n  \t\texp: jwt的过期时间，这个过期时间必须要大于签发时间\n  \t\tnbf: 定义在什么时间之前，该jwt都是不可用的.\n  \t\tiat: jwt的签发时间\n  \t\tjti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。\n2. 公共的声明\n\n\t公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.\n但不建议添加敏感信息，因为该部分在客户端可解密\n\n3. 私有的声明\n\n\t私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64\n\t是对称解密的，意味着该部分信息可以归类为明文信息。\n    \n\t这个指的就是自定义的claim。比如前面那个结构举例中的admin和name都属于自定的\n\tclaim。这些claim跟JWT标准规定的claim区别在于：JWT规定的claim，JWT的接收方在\n\t拿到JWT之后，都知道怎么对这些标准的claim进行验证(还不知道是否能够验证)；而\n\tprivate claims不会验证，除非明确告诉接收方要对这些claim进行验证以及规则才行。  \n\n定义一个payload: \n\n\t{\"sub\":\"1234567890\",\"name\":\"John Doe\",\"admin\":true}\n        \n然后将其进行base64编码，得到Jwt的第二部分。\n\n\teyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9\n    \n**签证（signature）** \n\njwt的第三部分是一个签证信息，这个签证信息由三部分组成：\n\n\theader (base64后的)\n\tpayload (base64后的)\n\tsecret\n\n这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符\n串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第\n三部分。\n\n\tTJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ\n    \n将这三部分用.连接成一个完整的字符串,构成了最终的jwt:\n\n\teyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6I\n\tkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7Hg\n\tQ\n\n注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用\n来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流\n露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。","tags":["jwt","token"],"categories":["Java"]},{"title":"常见的认证机制","url":"/2021/09/01/常见的认证机制/","content":"# HTTP Basic Auth\nHTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和\npassword，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供\n用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产环境下被\n使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP Basic\nAuth\n<!--more-->\n# Cookie Auth\nCookie认证机制就是为一次请求认证在服务端创建一个Session对象，同时在客户端\n的浏览器端创建了一个Cookie对象；通过客户端带上来Cookie对象来与服务器端的\nsession对象匹配来实现状态管理的。默认的，当我们关闭浏览器的时候，cookie会被删\n除。但可以通过修改cookie 的expire time使cookie在一定时间内有效；\n# OAuth\nOAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在\n某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和\n密码提供给第三方应用。\nOAuth允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提\n供者的数据。每一个令牌授权一个特定的第三方系统（例如，视频编辑网站)在特定的时\n段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这\n样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信\n息，而非所有内容\n\n![玖涯博客](/2021/09/01/常见的认证机/pasted-0.png)\n这种基于OAuth的认证机制适用于个人消费者类的互联网产品，如社交类APP等应\n用，但是不太适合拥有自有认证权限管理的企业应用。\n#  Token Auth\n使用基于 Token 的身份验证方法，在服务端不需要存储用户的登录记录。大概的流程是\n这样的：\n1. 客户端使用用户名跟密码请求登录\n2. 服务端收到请求，去验证用户名与密码\n3. 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端\n4. 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里\n5. 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token\n6. 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向\n客户端返回请求的数据\n\nToken机制相对于Cookie机制又有什么好处呢？\n- 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提\n是传输的用户认证信息通过HTTP头传输.\n- 无状态(也称：服务端可扩展行):Token机制在服务端不需要存储session信息，因为\nToken 自身包含了所有登录用户的信息，只需要在客户端的cookie或本地介质存储状态信息.\n-  更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，\nHTML,图片等），而你的服务端只要提供API即可.\n- 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在\n你的API被调用的时候，你可以进行Token生成调用即可.\n- 更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）\n时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认\n证机制就会简单得多。\n- CSRF:因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防\n范。\n-  性能: 一次网络往返时间（通过数据库查询session信息）总比做一次HMACSHA256\n计算 的Token验证和解析要费时得多.\n- 不需要为登录页面做特殊处理: 如果你使用Protractor 做功能测试的时候，不再需要\n为登录页面做特殊处理.\n- 基于标准化:你的API可以采用标准化的 JSON Web Token (JWT). 这个标准已经存在\n多个后端库（.NET, Ruby, Java,Python, PHP）和多家公司的支持（如：\nFirebase,Google, Microsoft）.\n","tags":["java","token"],"categories":["Java"]},{"title":"密码加密","url":"/2021/08/31/密码加密与JWT/","content":"#  BCrypt密码加密\n Spring Security提供了BCryptPasswordEncoder类,实现Spring的PasswordEncoder接口使用BCrypt强\n哈希方法来加密密码。\n<!--more-->\n\nBCrypt强哈希方法 每次加密的结果都不一样。\n\n1. 引入pom依赖\n\n  ```\n  <dependency>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-starter-security</artifactId>\n  </dependency>\n\n  ```\n2. 添加配置类\n\n\t我们在添加了spring security依赖后，所有的地址都被spring security所控制了，我们目\n\t前只是需要用到BCrypt密码加密的部分，所以我们要添加一个配置类，配置为所有地址\n\t都可以匿名访问\n   ```\n   /**\n    * 安全配置类\n    */\n    @Configuration\n    @EnableWebSecurity\n    public class WebSecurityConfig extends WebSecurityConfigurerAdapter{\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n                      http\n                      .authorizeRequests()\n                      .antMatchers(\"/**\").permitAll()\n                      .anyRequest().authenticated()\n                      .and().csrf().disable();\n                      }\n    }\n\n   ```\n3. 修改Application, 配置bean\n\t```\n    @Bean\n    public BCryptPasswordEncoder bcryptPasswordEncoder(){\n    return new BCryptPasswordEncoder();\n    }\n   ```\n4. 密码加密\n  ```\n    @Autowired\n    BCryptPasswordEncoder encoder;\n        public void add(Admin admin) {\n        admin.setId(idWorker.nextId()+\"\"); //主键值\n        //密码加密\n        String newpassword = encoder.encode(admin.getPassword());//加密后\n        的密码\n        admin.setPassword(newpassword);\n        adminDao.save(admin);\n    }\n  ```\n5. 密码校验\n\t\n    service添加\n    ```\n    /**\n    * 根据登陆名和密码查询\n    * @param loginname\n    * @param password\n    * @return\n    */\n    public Admin findByLoginnameAndPassword(String loginname, String\n    password){\n              Admin admin = adminDao.findByLoginname(loginname);\n              if( admin!=null && encoder.matches(password,admin.getPassword()))\n              {\n              return admin;\n              }else{\n              return null;\n              }\n    }\n\n    ```\n   controller添加\n   ```\n   /**\n    * 用户登陆\n    * @param loginname\n    * @param password\n    * @return\n    */\n    @RequestMapping(value=\"/login\",method=RequestMethod.POST)\n    public Result login(@RequestBody Map<String,String> loginMap){\n        Admin admin =\n        adminService.findByLoginnameAndPassword(loginMap.get(\"loginname\"),\n        loginMap.get(\"password\"));\n        if(admin!=null){\n        return new Result(true,StatusCode.OK,\"登陆成功\");\n        }else{\n        return new Result(false,StatusCode.LOGINERROR,\"用户名或密码错\n        误\");\n    }\n    }\n   ```\n ","tags":["密码加密"],"categories":["Java"]},{"title":"重装系统后精灵标记助手缓存找回","url":"/2021/08/24/重装系统后精灵标记助手缓存找回/","content":"在我一次冲动重装系统后，精灵标记助手之前标记的数据没有导出，导致数据无了。。。但是我又不甘心的找回来了，hhhhhhhhhhhhhhhh\n<!--more-->\n### 找回方法\n找到windows.old,我的路径是这个C:\\Windows.old\\Users\\Tai\\AppData\\Roaming\\Colabeler\n\n然后找到C:\\Users\\Tai\\AppData\\Roaming\\Colabeler，将数据覆盖就好，可以直接删掉复制"},{"title":"SSL VPN无法连接","url":"/2021/08/23/SSL-VPN无法连接/","content":"记一次惨痛的教训。\nSSL VPN 无法连接，获取不到ip资源的问题。\n<!--more-->\n安装上VPN后，虽然可以启动，但是VPN一会自己就掉了，而且无法连接ip地址。\n\n**先查看client.log**\n\n![玖涯博客](/2021/08/23/SSL-VPN无法连/pasted-0.png)\n\n捕捉到的信息\n```\n﻿ [INFOR][2021-08-22 21:44:19 187]Assigned IP: 3.0.1.6:255.255.254.0\n﻿ [ERROR][2021-08-22 21:44:19 344]TapdevOpen(): Cannot open device \\\\.\\Global\\{8EFE0A78-C826-4099-B0A9-26D78E0A3FDB}.tap: 2\n﻿ [ERROR][2021-08-22 21:44:19 375]Failed to open TAP device,So sleep a little time and try again!\n﻿ [ERROR][2021-08-22 21:44:20 626]TapdevOpen(): Cannot open device \\\\.\\Global\\{8EFE0A78-C826-4099-B0A9-26D78E0A3FDB}.tap: 2\n﻿ [ERROR][2021-08-22 21:44:20 648]Failed to open TAP device,So sleep a little time and try again!\n﻿ [ERROR][2021-08-22 21:44:21 869]TapdevOpen(): Cannot open device \\\\.\\Global\\{8EFE0A78-C826-4099-B0A9-26D78E0A3FDB}.tap: 2\n﻿ [INFOR][2021-08-22 21:44:21 928]Sslvpn client clean all and exit normally!\n```\n**查看user.log**\n\n![玖涯博客](/2021/08/23/SSL-VPN无法连/pasted-1.png)\n这么看没问题，但是和正常的user.log对比\n\n![玖涯博客](/2021/08/23/SSL-VPN无法连/pasted-2.png)\n没有请求id资源。\n我只知道有设备没有启动但是不知道是哪个，因为有一个vpn可以运行所以一开始就排除了虚拟网卡，但是问题就是虚拟网卡。\n\n因为解决不了问题，脑子一热就重装了系统，但是没想到我用精灵标注标注的800张图片，竟然缓存在C盘，所以我花了十多天标的数据无了。。。。。。(后续就是，我在windows.old里找回来了)\n\n重装系统后问题还是没有解决。所以我寻找TAP设备，打开设备管理器。\n\n![玖涯博客](/2021/08/23/SSL-VPN无法连/pasted-3.png)\n当时TAP是黄色的，显示签名有问题。\n\n所以应该是它的问题。\n\n解决办法是进bios，关掉secure boot.","tags":["SSL VPN"]},{"title":"Java笔试","url":"/2021/08/11/Java笔试/","content":"## 1.final应用场景\n- 用来修饰数据，包括成员变量和局部变量，该变量只能被赋值一次且它的值无法被改变。对于成员变量来讲，我们必须在声明时或者构造方法中对它赋值；\n- 用来修饰方法参数，表示在变量的生存期中它的值不能被改变；\n- 修饰方法，表示该方法无法被重写；\n- 修饰类，表示该类无法被继承。\n<!--more-->\n\n## 2.Redis数据结构\nRedis 有 5 种基础数据结构，它们分别是：string(字符串)、list(列表)、hash(字典)、set(集合) 和 zset(有序集合)。\n\n## 3.hashmap和treemap查找效率 特性对比\n- hashmap是无序的，treemap是有序的，整个key是按照自然顺序来的。\n- hashmap可以put一个null当key ，treemap却不支持。\n- 底层结构不一样，一个是数组➕红黑树，一个直接就是红黑树。\n- TreeMap取出来的是排序后的键值对。插入、删除需要维护平衡会牺牲一些效率。但如果要按自然顺序或自定义顺序遍历键，那么TreeMap会更好。\n- HashMap比TreeMap的效率要高\n\n## 4.Redis持久化\nRedis所有数据保存在内存中，对数据的更新将异步地保存到磁盘上，使得数据在Redis重启之后仍然存在。\n\n Redis提供了两种不同的持久化方法将数据保存到硬盘里面。\n\n 快照持久化：将Redis某一时刻存在的所有数据都写入硬盘。\nRedis通过创建快照来获得存储在内存里面的数据在某个时间节点上的副本。\n\n AOF持久化：AOF的全称叫append-only file，中文意思是只追加文件。当使用AOF持久化方式的时候，Redis执行写命令的时候，将被执行的写命令复制到硬盘里面，说的通俗一点就是写日志。\n\n AOF持久化将被执行的写命令写到AOF文件的末尾，以达到记录数据的目的。Redis只要从头到尾重新执行一次AOF所有的命令就可以恢复数据。\n \n## 5.MySQL索引\n索引是一个单独的、存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针。使用索引用于快速找出在某个或多个列中有一特定值的行，所有MySQL列类型都可以被索引，对相关列使用索引是提高查询操作速度的最佳途径。\n\n# 6.原子性\n原子性是指一个操作是不可中断的，要么全部执行成功要么全部执行失败，有着“同生共死”的感觉。及时在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程所干扰。\n\n# 7.引用类型\n在 JDK.1.2 之后，Java 对引用的概念进行了扩充，将引用分为了：强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4 种，这 4 种引用的强度依次减弱。\n## 强引用\nJava默认的声明就是强引用，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足时，JVM也会直接抛出OutOfMemoryError，不会去回收。如果想中断强引用与对象之间的联系，可以显示的将强引用赋值为null，这样一来，JVM就可以适时的回收对象了\n## 软引用\n软引用是用来描述一些非必需但仍有用的对象。在内存足够的时候，软引用对象不会被回收，只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。这种特性常常被用来实现缓存技术，比如网页缓存，图片缓存等。\n\n在 JDK1.2 之后，用java.lang.ref.SoftReference类来表示软引用。\n## 弱引用\n弱引用的引用强度比软引用要更弱一些，无论内存是否足够，只要 JVM 开始进行垃圾回收，那些被弱引用关联的对象都会被回收。在 JDK1.2 之后，用 java.lang.ref.WeakReference 来表示弱引用。\n## 虚引用\n虚引用是最弱的一种引用关系，如果一个对象仅持有虚引用，那么它就和没有任何引用一样，它随时可能会被回收，在 JDK1.2 之后，用 PhantomReference 类来表示，通过查看这个类的源码，发现它只有一个构造函数和一个 get() 方法，而且它的 get() 方法仅仅是返回一个null，也就是说将永远无法通过虚引用来获取对象，虚引用必须要和 ReferenceQueue 引用队列一起使用。\n","tags":["Java笔试"],"categories":["Java"]},{"title":"Nginx用途","url":"/2021/08/10/Nginx用途/","content":"# 一、静态代理\n# 二、负载均衡\n# 三、限流\n# 四、缓存\n# 五、黑白名单","tags":["Java","Nginx"],"categories":["Java"]},{"title":"RS Loss","url":"/2021/08/09/RS-Loss/","content":"本文作者提出了一种用于目标检测和实例分割任务的Rank & Sort Los），可以用来涨点。\n\n在多阶段的目标检测器上，RS Loss在标准的Faster R-CNN上达到39.6 AP，并且超过了FPN 3.4AP，aLRP 2.2AP，Dynamic R-CNN 0.7AP。\n\n与Focal Loss比较,当两个网络在没有辅助head的情况下训练时，RS Loss提供大约 1 AP的增益。\n<!--more-->\n\n论文地址：https://arxiv.org/abs/2107.11669\n\n代码地址：https://github.com/kemaloksuz/RankSortLoss\n\nRS Loss其实由两部分组成，一部分是Rank，还有一部分是Sort。Rank指的是根据Classification Lofits区分出正负的样本，Sort指的是根据IoU来对正样本进行排序。\n### 好处\n- 因为正样本根据IoU进行Sort，不同的正样本在训练的时候就会有不同的优先级，所以用RS Loss训练的检测器就不再需要额外的辅助head 了。\n- 由于RS Loss根据Classification Lofits区分出正负的样本，因此用RS Loss训练的检测器就能够处理比较极端的不平衡数据分布 ，而不需要采用启发式的采样。\n- 此外，除了学习率，RS Loss不需要任何超参数调优 ，因为在RS Loss中没有需要调优的任务平衡系数。\n## 损失函数定义\n![玖涯博客](/2021/08/09/RS-Los/image1.png)\n1. 直接衡量了目标和期望误差之间的差异，产生一个可解释的损失值，解决了D1的问题；\n\n2. 不需要限制i是数据集中的正样本，这样的定义更加完整。\n## 损失函数计算\n\n![玖涯博客](/2021/08/09/RS-Los/image2.png)\n\n## 损失函数优化\n\n![filename already exists, renamed](/2021/08/09/RS-Los/pasted-0.png)\n\n在原始的检测任务中，损失函数通常由三部分组成：\n\n![玖涯博客](/2021/08/09/RS-Los/image3.png)\n其中cls为 Focal Loss，box为  GIoU Loss ，str为 Cross-entropy Loss。\n\n在本文中作者首先砍掉了centerness head，然后 RS Loss代替了，得到：\n\n![玖涯博客](/2021/08/09/RS-Los/image.png)\n\n## 比较图\n![玖涯博客](/2021/08/09/RS-Los/image4.png)\n在上图中，(a)是一个以前的Visual Detector的流程，包括可能来自多个阶段的多个子任务的head；（b）是使用RS Loss进行训练的Visual Detector；（c）展示了如何通过loss的值来平衡每个任务的权重，从而避免了调参。","tags":["Loss"],"categories":["深度学习"]},{"title":"RequestBody和RequestParam","url":"/2021/08/04/RequestBody和RequestParam/","content":"RequestParam接收的是Json对象，RequestBody接收的是Json字符串\n<!---->\n\n通常Ajax传的是Json对象，所以后台可以使用RequestParam Map<String,String> map接收，如果想使用RequestBody Map<String,String> map ，要使用JSON.stringify(data)的方式就能将对象变成字符串。同时ajax请求的时候也要指定dataType: \"json\",contentType:\"application/json\" 这样就可以轻易的将一个对象或者List传到Java端，使用@RequestBody即可绑定对象或者List.","tags":["Java"],"categories":["Java"]},{"title":"CANet中出现ValueError: could not broadcast input array from shape (x,5) into shape (100,5)","url":"/2021/07/20/CANet中出现ValueError-could-not-broadcast-input-array-from-shape-x-5-into-shape-100-5/","content":"当用CANet运行自己的数据集的时候会出现ValueError: could not broadcast input array from shape (x,5) into shape (100,5)\n原因是标签数量问题，修改configure.py 文件中的MAX_NUM_GT_BOXES为你最大的标签数量","tags":["CANet"],"categories":["深度学习"]},{"title":"ElasticSearch","url":"/2021/07/18/ElasticSearch/","content":"Elasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速\n度去处理大规模数据。ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分\n布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发\n的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用\n于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。\n<!--more-->\n\n# ElasticSearch体系结构\n\n\n![玖涯博客](/2021/07/18/ElasticSearc/image.png)\n\n#  ElasticSearch部署\n\n下载ElasticSearch 5.6.8版本\nhttps://www.elastic.co/downloads/past-releases/elasticsearch-5-6-8\n\n无需安装，解压安装包后即可使用\n\n在命令提示符下，进入ElasticSearch安装目录下的bin目录,执行命令\n\n```\nelasticsearch\n```\n即可启动。\n\n我们打开浏览器，在地址栏输入http://127.0.0.1:9200/ 即可看到输出结果\n\n#  IK分词器\n## 什么是IK分词器\n我们在浏览器地址栏输入http://127.0.0.1:9200/_analyze?\nanalyzer=chinese&pretty=true&text=我是程序员，浏览器显示效果如下\n```\n{\n\"tokens\" : [\n{\n\"token\" : \"我\",\n\"start_offset\" : 0,\n\"end_offset\" : 1,\n\"type\" : \"<IDEOGRAPHIC>\",\n\"position\" : 0\n},\n{\n\"token\" : \"是\",\n\"start_offset\" : 1,\n\"end_offset\" : 2,\n\"type\" : \"<IDEOGRAPHIC>\",\n\"position\" : 1\n},\n{\n\"token\" : \"程\",\n\"start_offset\" : 2,\n\"end_offset\" : 3,\n\"type\" : \"<IDEOGRAPHIC>\",\n\"position\" : 2\n},\n{\n\"token\" : \"序\",\n\"start_offset\" : 3,\n\"end_offset\" : 4,\n\"type\" : \"<IDEOGRAPHIC>\",\n\"position\" : 3\n},\n{\n\"token\" : \"员\",\n\"start_offset\" : 4,\n\"end_offset\" : 5,\n\"type\" : \"<IDEOGRAPHIC>\",\n\"position\" : 4\n}\n]\n}\n\n```\n\n默认的中文分词是将每个字看成一个词，这显然是不符合要求的，所以我们需要安装中\n文分词器来解决这个问题。\n\nIK分词是一款国人开发的相对简单的中文分词器。虽然开发者自2012年之后就不在维护\n了，但在工程应用中IK算是比较流行的一款！\n\n## IK分词器安装\n下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases 下载5.6.8版\n本\n- 将ik文件夹拷贝到elasticsearch/plugins 目录下。\n- 重新启动，即可加载IK分词器\n\nIK提供了两个分词算法ik_smart 和 ik_max_word\n其中 ik_smart 为最少切分，ik_max_word为最细粒度划分\n\n## 自定义词库\n- 进入elasticsearch/plugins/ik/config目录\n- 新建一个my.dic文件，编辑内容：\n\n```\n河北工业大学\n```\n- 修改IKAnalyzer.cfg.xml（在ik/config目录下）\n\n```\n<properties>\n\t<comment>IK Analyzer 扩展配置</comment>\n\t<!‐‐用户可以在这里配置自己的扩展字典 ‐‐>\n\t<entry key=\"ext_dict\">my.dic</entry>\n\t<!‐‐用户可以在这里配置自己的扩展停止词字典‐‐>\n\t<entry key=\"ext_stopwords\"></entry>\n</properties>\n```\n- 重新启动elasticsearch\n\n","tags":["Java","ElasticSearch"],"categories":["Java"]},{"title":"MongoDB","url":"/2021/07/15/MongoDB/","content":"MongoDB 是一个跨平台的，面向文档的数据库，是当前 NoSQL 数据库产品中最热门\n的一种。它介于关系数据库和非关系数据库之间，是非关系数据库当中功能最丰富，最\n像关系数据库的产品。它支持的数据结构非常松散，是类似 JSON 的 BSON 格式，因此可以\n存储比较复杂的数据类型。\n主要用于吐槽，评论等数据的保存\n主要是用于 数据流量大而且可以丢失少量数据\n<!--more-->\n\n# 引入依赖\n```\n<dependencies>\n<dependency>\n  <groupId>org.mongodb</groupId>\n  <artifactId>mongodb-driver</artifactId>\n  <version>3.6.3</version>\n  </dependency>\n</dependencies>\n\n```\n# 创建application.yml\n```\nserver:\n\tport: 9006\nspring:\n\tapplication:\n\t\tname: tai-spit #指定服务名\n\tdata:\n\t\tmongodb:\n\t\t\thost: 192.168.9.2\n\t\t\tdatabase: spitdb\n\n```\n\n# 创建启动类\n```\n@SpringBootApplication\npublic class SpitApplication {\n  public static void main(String[] args) {\n      SpringApplication.run(SpitApplication.class, args);\n  }\n}\n\n```\n# 再创建自己的实体类\n# 创建Dao\n```\npublic interface SpitDao extends MongoRepository<Spit, String>{\n}\n\n```\n# 再创建service层，在service里面使用springdata的增删改查就可以了","tags":["java","springDataMongoDB"],"categories":["Java"]},{"title":"SpringDataRedis","url":"/2021/07/15/SpringDataRedis/","content":"最近做springcloud项目，接触到了springDataRedis\n<!--more-->\n\nSpring-data-redis是spring大家族的一部分，提供了在srping应用中通过简单的配置访问\nredis服务，对reids底层开发包(Jedis, JRedis, and RJC)进行了高度封装，RedisTemplate\n提供了redis各种操作。\n\n首先导入依赖\n```\n<dependency>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-data-redis</artifactId>\n</dependency>\n```\n修改application.yml ,在spring节点下添加配置\n\n\tredis:\n\t\thost: 192.168.9.2\n      \n修改ArticleService 引入RedisTemplate，并修改findById方法\n```\n@Autowired\nprivate RedisTemplate redisTemplate;\n/**\n* 根据ID查询实体\n* @param id\n* @return\n*/\npublic Article findById(String id) {\n              //从缓存中提取\n              Article article=\n              (Article)redisTemplate.opsForValue().get(\"article_\"+id);\n              // 如果缓存没有则到数据库查询并放入缓存\n              if(article==null) {\n              article = articleDao.findById(id).get();\n              redisTemplate.opsForValue().set(\"article_\" + id, article);\n              }\n              return article;\n}\n\n```\n这样在查询的时候，就会自动将文章放入缓存\n\n修改或删除后清除缓存\n```\n    /**\n    * 修改\n    * @param article\n    */\n    public void update(Article article) {\n        redisTemplate.delete( \"article_\" + article.getId() );//删除缓存\n        articleDao.save(article);\n    }\n    /**\n    * 删除\n    * @param id\n    */\n    public void deleteById(String id) {\n        redisTemplate.delete( \"article_\" + id );//删除缓存\n        articleDao.deleteById(id);\n    }\n```\n缓存过期处理\n```\nredisTemplate.opsForValue().set(\"article_\" + id, article,1,TimeUnit.DAYS);\n```","tags":["Java","springDataRedis"],"categories":["Java"]},{"title":"Distilling Object Detectors via Decoupled Features","url":"/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Features/","content":"本次工作作者提出一种简单而高效的通过解耦特征进行目标检测的蒸馏方法。通过分析并证明了背景区域在蒸馏过程中的重要作用。后引入 DeFeat 方法，在该方法中，特征在 FPN 水平和 RoI-aligned 特征水平上被分割成物体和背景部分，并对这两部分分别进行蒸馏处理。\n\nDeFeat 具有很强的泛化能力，可以很容易地用于one-stage 和 two-stage 的检测框架。大量实验也验证了 DeFeat 的有效性，并且超越了最先进的目标检测的蒸馏方法。例如，DeFeat 将基于 ResNet50 的 Faster R-CNN的 mAP 从 37.4% 提高到 40.9%，并将基于 ResNet50 的RetinaNet 在COCO 基准上的 mAP 从 36.5% 提高到 39.7%。\n\n作者 | Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu, Chang Xu\n\n<!--more-->\n\n**Abstract**\n\n知识提炼是从复杂的教师网络继承信息到紧凑的学生网络并保持强大性能的一种广泛使用的范式。与图像分类不同，目标检测器更加复杂，具有多个损失函数，其中语义信息所依赖的特征错综复杂。在本文中，我们指出从排除对象的区域中得到的特征信息对于提取学生检测器也是必不可少的，这在现有的方法中通常被忽略。此外，我们阐明了在蒸馏过程中，来自不同区域的特征应被赋予不同的重要性。为此，我们提出了一种新的蒸馏算法，该算法通过解耦特征(FAuLT)来学习更好的学习检测器。具体来说，将处理两个层次的解耦特征，以便将有用信息嵌入到学生中，即，从颈部解耦的特征和从分类头解耦的建议。在具有不同主干的各种检测器上的大量实验表明，所提出的able能够超越最先进的蒸馏方法用于物体检测\n\n# 1. Introduction\n作为计算机视觉的基本任务之一，目标检测在包括自动驾驶和监控视频分析在内的各种现实应用中受到越来越多的关注。深度学习的最新进展引入了许多基于卷积神经网络的目标检测解决方案。检测器的主干通常由大量卷积运算组成，以产生对检测至关重要的密集特征准确性。但是这样做不可避免地会导致计算资源成本的急剧增加和检测速度的明显下降。诸如量化[19，58，31，57，62]、剪枝[2，17，20]、网络设计[55，49，15，18]和知识提炼[56，6]等技术已经被开发来克服这种困境并实现对检测任务的有效推断。我们对知识提炼特别感兴趣[24]，因为它提供了一种优雅的方式来学习一个紧凑的学生网络，当性能证明的教师网络可用时。经典的知识提取方法首先被开发用于分类任务，以决定图像属于哪个类别。来自优化良好的教师网络的软标签输出[24，28，38，13]或中间特征[1，23，66]的信息已经被很好地利用来学习学生网络，但是这些方法不能直接扩展到需要进一步找出对象在哪里的检测任务。\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image.png)\n在目标检测任务中有一些研究知识提取的尝试。例如，FGFI [56]要求学生网络在近物体锚位置上模仿教师网络。TADF [47]通过颈部特征中的高斯掩蔽目标区域和检测头中的阳性样本提取学生。这些工作仅从目标区域提取知识，因为背景区域在检测任务中不被认为是感兴趣的。直观地说，在蒸馏过程中，背景区域可能会引入大量噪声，并且很少被探索。但是在进行蒸馏时缺乏对背景区域的全面分析。因此，抛弃背景区域的草率决定可能并不明智。最重要的是，背景信息已经被证明有助于视觉识别[53，46，9，16]。与其猜测背景区域对蒸馏无用甚至有害，不如对背景进行公平彻底的分析，让事实说话.\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image1.png)\n我们首先通过比较两种方法来检验对象和背景区域在知识提取中的作用:(1)仅通过对象区域FPN特征提取和(2)仅通过背景区域FPN特征提取。当通过来自教师检测器的背景区域提取时，学生不会被显著增强，这是理所当然的，因为背景的信息量和噪声较小[56]。然而，在各种模型和数据集上进行大量实验后，我们观察到一个令人惊讶的结果，即仅通过背景区域特征提取学生也可以显著增强学生，甚至获得与通过对象区域提取相似的结果(图4)。我们通过提取背景特征进一步探索性能提升的来源。以COCO的两个类为例(见图1)，我们进行了误差分析[25]，发现通过背景区域的蒸馏有效地减少了背景误报的数量。上述证据表明，背景区域实际上可以作为对目标区域提取的补充。除此之外，先前的文献已经表明，物体和背景之间有很强的关系[53，69]。目标可能性[53]可以写成\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image2.png)(Vo和Vb分别是对象区域和背景的特征）**所有的概率都与背景信息有关，背景信息提供了找到一个物体的可能性的估计(例如，一个人不可能在房间里找到一辆汽车)。**基于背景的先验在不同的图像中有所不同[69]，因此我们需要学习背景特征以进行更好的预测。然而，上述有希望的期望未能被以前的工作证明是正确的[6，30]，这些工作同时考虑了目标和背景区域。虽然他们利用了这两种类型的区域，但与那些只使用对象区域的学生相比，学生并没有显著提高，这似乎与[56]所指出的现象一致。无论是目标区域还是背景区域都可以通过蒸馏独立地有利于目标检测，但是一旦它们被集成在一起，性能会意外下降。原因可能是他们的方法直接集成了这两种类型的区域。从梯度的角度来看，我们在图2中说明了对象和背景区域之间的不一致性。左列图像是从COCO训练集中随机选取的，右列图像是它们在学生检测器中对应的颈部特征梯度。我们可以观察到，来自对象区域的梯度幅度始终大于来自背景区域的梯度幅度。因此，这提醒我们在蒸馏过程中目标区域和背景区域的不同重要性。\n\n基于这些深刻的观察，我们建议分离用于知识提炼的特征，并强调它们在提炼过程中的独特重要性。包括两个级别的功能，即FPN功能和RoI对齐功能。使用地面真值遮罩将FPN特征分割为对象和背景部分，并在教师和学生之间应用均方误差损失。与RoI对齐的特征也使用教师的预测区域建议解耦为正部分和负部分。基于这些解耦的RoI对齐特征生成的分类逻辑是使用KL散度损失提取的。由此产生的BulT算法可以自适应地结合到一级和两级检测器中，以提高检测精度。\n\n为了验证我们的方法，我们在各种场景下对fast R-CNN[43]和retianet[34]进行了广泛的实验，包括在两个常见的检测基准PASCAL VOC [12]和COCO [35]上对浅学生和窄学生进行蒸馏。特别是，在COCO基准测试中，我们的failure将基于ResNet50的FPN从37.4%提高到40.9% mAP，将基于ResNet50的RetinaNet从36.5%提高到39.7% mAP。\n\n# 2. Related Work\n**Object detection** 被认为是最具挑战性的视觉任务之一，该任务旨在发现当给定图像时物体在什么地方。在过去几年中，单级[42，36，34，29，11，67]和双级[43，21，33，26，27，5]探测器的精度都有了显著提高。尽管安装了非常深的主干[59，48]的检测器具有更好的检测精度，但它们在计算成本方面很昂贵，并且很难部署到移动设备上。有一个有趣的研究方向是通过权重量化来压缩大型检测模型[31，57]，用更少的比特来表示参数权重。修剪[37，14，60，52，51]是另一个研究方向，它从一个大的预训练模型中移除不重要的连接来压缩检测器。设计与轻量级主干网络[55，41，45，32，61，64]相结合的检测器也是检测速度更快的趋势。此外，还有一个研究方向是将知识从一个大检测器转移到一个小检测器[6，56，30]，在这个方向上，人们可以在不设计新架构的情况下提升小检测器的性能。\n\n**Knowledge distillation**(KD)已经成为将大模型压缩成更小更快模型的最有效技术之一。KD最早由Buciluˇ a等人[4]提出，由Hinton等人[24]推广，通过软输出将教师网络的暗知识传递给学生网络。FitNets [44]表明，激活[23]和中间层的特征[39]也可以被视为指导学生网络的知识。此后，KD被广泛应用于分类任务[22，63，3，54，7，65，10]。最近，有几项工作提出用知识蒸馏压缩目标检测器。Chen等人[6]通过所有组件(即颈部特征、分类头和回归头)提取学生，但是整个特征图的模仿和分类头中的提取都忽略了前景和背景的不平衡，这可能导致次优的结果。唐等[50]提出了一级检测器的自适应蒸馏损失，以放大硬样品的损失。Li等人[30]从区域提议中提取了采样特征，但是，仅模仿上述区域可能会导致误导，因为提议有时表现不佳。王等人[56]打算从前景对象区域提取具有细粒度特征的学生。然而，我们发现剩余的背景特征对于提取更好的学生检测器也是至关重要的。\n\n**总之，当前的目标检测的蒸馏框架忽略了背景区域在中间特征中的重要作用和分类头中的负区域建议。在这项工作中，我们发现FPN特征中的目标区域和背景区域对于蒸馏都是实用的，并且平等地对待正面和负面建议会抑制检测器更强的性能。因此，我们首先生成一个二进制掩码来分离中间特征，然后提取相应的特征。同时，我们将分类头中的正负建议解耦，进一步提高泛化能力。**\n\n# 3. Distillation via Decoupled Features\n通常，对象检测器由三个或四个组件组成:(a)用于提取语义特征的主干；融合多层次特征的瓶颈；(c)用于生成建议书的远程过程网络(仅在两级探测器中)；以及(d)用于对象分类和包围盒回归的头部。提炼的目的是向学生灌输教师的黑暗知识，这些知识可以是中间层的特征，也可以是分类头中区域建议的软预测。将S ∈RH×W×Cand T ∈RH×W×C分别定义为学生和教师的中间特征。经由中间特征的蒸馏可以被表述为:\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image.3png.png)\n其中N = HW C为元素总数，γ用于控制蒸馏损失的尺度，φ表示适配层[6]，I表示仿掩模，即[47]中的高斯掩模和[56]中的细粒度掩模。在以前的工作中，只考虑对象区域或均匀提取整个特征图。均匀处理所有区域的方法[6，30]中的蒙版可以看作是全一张量。\n\n给定从RPN输出的K区域建议，分类负责人需要计算所有建议的软标签。通过软预测的蒸馏可以表述为:\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image4.png)\n其中超参数λ用于平衡不同的损失项，lce和lklde分别记下交叉熵损失和KL散度损失。Yiis第一个提议的基本事实标签，以及学生和教师探测器分别是ys和yt i。学生的总体培训目标可以表述为:\n\n![filename already exists, renamed](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/pasted-0.png)\n其中，Lregis表示探测器头中的边界框回归损失，RPN表示两级探测器中的RPN损失。\n\n## 3.1. Decouple Intermediate Features in Distillation\n以前的作品要么选择部分区域，要么使用所有区域，但平等对待中间要素上的每个位置。特别是，FGFI [56]假定背景区域会引入大量噪声，并会损害性能。然而，这种直觉判断与我们在实验中观察到的并不一致，如图1所示。经由仅背景区域的蒸馏仍然获得与经由仅目标区域的蒸馏相当的结果。我们得出结论，中间特征中的背景区域可以补充目标区域，进一步帮助学生检测器的训练，但剩下的问题是如何在蒸馏中适当地整合这两种类型的区域。\n\n基于以上观察，我们建议通过解耦特征提取学生。给定大小为H × W的中间特征，我们首先根据地面真值框B生成一个二进制掩码M:.\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image5.png)\n其中M∑{ 0，1}H×W，如果属于某个对象，则位置(I，j)的值为1，否则为0。具体来说，如果检测器包含可以输出多级特征的特征金字塔网络(FPN)，我们将为每个地面真值框分配其对应的级别，并相应地为每个级别生成掩码M。然后，我们使用生成的二进制掩码来分离颈部特征，如图3所示。中间特征蒸馏公式如下:\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image6.png)\n\n## 3.2. Decouple Region Proposals in Distillation\n通过软预测进行知识提取已经广泛应用于分类任务，并且可以用于检测任务中分类头的提取。然而，不同于训练时没有背景类别的分类任务(如CIFAR和ImageNet)，检测头中的对象和背景类别可以有极其不同数量的建议。我们进行实验来探索如图6所示的目标(正)提议和背景(负)提议的分离蒸馏损失。积极提案的提炼损失始终大于消极提案。如果它们没有得到适当的平衡，背景提议产生的小梯度会淹没在正梯度产生的大梯度中，从而限制了进一步的细化。此外，表5显示，与仅使用否定建议相比，平等对待所有建议会得到更差的结果。因此，我们建议在提取分类头时，将区域建议解耦为正建议和负建议，以实现最优收敛。我们将教师检测器生成的区域建议输入到教师和学生的头部，生成如图3所示的类别预测标准。在我们的方法中，正面建议和负面建议是分开处理的。考虑到积极建议的逻辑，我们通过给老师和学生一个温度来软化预测，如下所示:\n\n![filename already exists, renamed](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/pasted-1.png)\n其中θs和θt分别代表学生和老师的参数。Y = {1，2，...，C}是检测基准的类别。对于属于背景区域的建议，我们通过类似于上述等式的教师和学生的温度Tbgfor来软化预测。为了从老师的检测器中提取学生的知识，我们使用了如下公式:\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image8.png)\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image9.png)\n通过解耦特征(FAuLT)框架提出的蒸馏概述。我们将中间FPN特征中的区域和来自RPN的区域建议解耦，以提取学生检测器。术语“分数(pos)”和“分数(neg)”分别表示正面和负面建议的分类分数。\n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image10.png)\n\n# 4. Experiments\n## 4.1. Datasets and Metrics\nCOCO [35]是一个具有挑战性的对象检测基准，它包含80个对象类。我们的训练集是80k个训练图像和35k个验证图像子集(trainval35k)的并集，而验证集是剩余的5k个验证图像(minival)。我们将平均精度视为评估指标，即mAP、AP50、AP75、APS、APMand和APL。最后三个测量不同比例的物体的性能\n\nPascal VOC [12]包含20个对象类。我们的训练集是VOC 2007 trainval (5K)和VOC 2012 trainval (11K)的并集，验证集是VOC 2007测试(4.9K)。我们使用0.5的IoU报告mAP分数。\n\n## 4.2. Implementation Details\n所有实验均在8个TeslaV100 GPUs上进行。我们的实现基于带有Pytorch框架[40]的mmdetection [8]。We use SGD optimizer with a batch size of 4 images per GPU, all models are trained for 12\nepochs, known as 1× schedule. \n\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image11.png)调整输入图像的大小，使其短边在COCO上有800个像素，在VOC上有600个像素。FPN [33]和RetinaNet[34]的初始学习率分别设置为0.04和0.02。And the learning rate is divided by 10 at the 8-th and 11-th epochs.\nWe set momentum as 0.9 and weight decay as 0.0001.\n![玖涯博客](/2021/07/14/Distilling-Object-Detectors-via-Decoupled-Feature/image12.png)\n\n## 4.3. Main Results\n我们首先在COCO [35]基准上验证我们提出的典型两阶段检测框架FPN [33]上的Table的有效性，如表1所示。位于FPN的ResNet50被选为学生检测器，位于FPN的ResNet152被选为教师检测器。“全颈”表示学生是通过平等对待FPN所有地区的特征而被蒸馏出来的，如等式1所示。“解耦”是指通过解耦的FPN特征提取学生，如等式5所示。“全cls”表示所有地区提案在提炼过程中得到同等对待。“解耦-cls”表示区域建议被解耦为正(对象)和负(背景)，如等式8所示。“主干”是指主干特征也被提炼出来。直接提取FPN所有区域的特征达到40.1%的mAP，解耦后的FPN特征可以进一步提高学生检测器0.3%的mAP。通过解耦的FPN特征和RPN建议提取的学生获得了更高的结果，并且我们的解耦建议可以将结果从40.5%提升到40.8% mAP。在蒸馏中进一步采用主干特性将在COCO基准上实现40.9%的mAP，与学生基线模型相比带来3.5%的收益。此外，我们还在典型的一级检测框架retainet上进行了实验[34]，我们的resnet 152-R50-retainet在COCO上将基线对应的mAP从36.5%提高到39.7%。这些结果清楚地阐明了我们提出的一级和二级探测器的多功能性和通用性。\n\n4.4往后没啥可翻译的\n\n# 5. Conclusion\n\n在这篇文章中，我们提出了一个简单而有效的方法，通过解耦特征提取目标检测。我们分析和论证了背景区域在蒸馏过程中的重要作用。在大量观察的基础上，我们引入了feature方法，该方法在FPN级和RoI对齐特征级将特征分解为目标和背景部分，并分别对这两部分进行提取。failure是通用的，可以很容易地用于一阶段和两阶段检测框架。大量的实验通过始终优于其他蒸馏技术验证了FAuLT的有效性。\n","tags":["目标检测"],"categories":["深度学习"]},{"title":"数组中只出现一次的数","url":"/2021/07/13/数组中只出现一次的数/","content":"**描述**\n\n给定一个整型数组 arr 和一个整数 k(k>1)。\n已知 arr中只有 1 个数出现一次，其他的数都出现 k次。\n请返回只出现了 1 次的数。\n<!--more-->\n\n**代码**\n\n```\n    \n    public int foundOnceNumber (int[] arr, int k) {\n        // write code here\n        int[] binarySum = new int[32];\n        for(int i = 0;i < 32;i++){\n            int sum = 0;\n            for(int num : arr){\n                sum += (num >> i & 1);\n            }\n            binarySum[i] = sum;\n        }\n        int res = 0;\n        for(int i = 0;i<32;i++){\n            if(binarySum[i] % k != 0){\n                res += 1 << i;\n            }\n        }\n        return res;\n        }\n```","tags":["java"],"categories":["java"]},{"title":"测试","url":"/2021/07/12/测试/","content":"测试文档\n","tags":["测试"]}]