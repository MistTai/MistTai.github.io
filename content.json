{"meta":{"title":"秘密基地","subtitle":"","description":"","author":"tai","url":"http://tai.social","root":"/"},"pages":[{"title":"categories","text":"","path":"categories/index.html","date":"07-12","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"07-12","excerpt":""}],"posts":[{"title":"缓存处理","text":"在Java开发项目中，为了提高查询的性能，一般采用Redis缓存解决。 Redis环境搭建 以docker的形式搭建Redis服务 1docker run -di --name=tensquare_redis -p 6379:6379 redis SpringDataRedis Spring-data-redis是spring大家族的一部分，提供了在srping应用中通过简单的配置访问redis服务，对reids底层开发包(Jedis, JRedis, and RJC)进行了高度封装，RedisTemplate提供了redis各种操作。 先通过pom.xml引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 修改application.yml，在spring节点下添加配置12redis: host: 192.168.184.134 引入RedisTemplate,并加入缓存 12345678910111213141516171819 @Autowired private RedisTemplate redisTemplate;/*** 根据ID查询实体* @param id* @return*/public Article findById(String id) &#123; //从缓存中提取 Article article= (Article)redisTemplate.opsForValue().get(&quot;article_&quot;+id); // 如果缓存没有则到数据库查询并放入缓存 if(article==null) &#123; article = articleDao.findById(id).get(); redisTemplate.opsForValue().set(&quot;article_&quot; + id, article); &#125; return article;&#125; 修改或删除后清除缓存 1234567891011121314151617/*** 修改* @param article*/public void update(Article article) &#123; redisTemplate.delete( &quot;article_&quot; + article.getId() );//删除缓存 articleDao.save(article);&#125;/*** 删除* @param id*/public void deleteById(String id) &#123; redisTemplate.delete( &quot;article_&quot; + id );//删除缓存 articleDao.deleteById(id);&#125; 缓存过期处理 设置一天过期时间 12redisTemplate.opsForValue().set(&quot;article_&quot; + id, article,1,TimeUnit.DAYS); Spring Cache Spring Cache使用方法与Spring对事务管理的配置相似。Spring Cache的核心就是对某个方法进行缓存，其实质就是缓存该方法的返回结果，并把方法参数和结果用键值对的方式存放到缓存中，当再次调用该方法使用相应的参数时，就会直接从缓存里面取出指定的结果进行返回。 @Cacheable——-使用这个注解的方法在执行后会缓存其返回结果。@CacheEvict——–使用这个注解的方法在其执行前或执行后移除Spring Cache中的某些元素。 参考这篇博客https://www.cnblogs.com/fashflying/p/6908028.html","path":"2021/12/29/缓存处理/","date":"12-29","excerpt":"在Java开发项目中，为了提高查询的性能，一般采用Redis缓存解决。","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://tai.social/tags/Spring-Cloud/"},{"name":"缓存","slug":"缓存","permalink":"http://tai.social/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"分布式ID生成器","text":"由于我们的数据库在生产环境中要分片部署（MyCat）,所以我们不能使用数据库本身的自增功能来产生主键值，只能由程序来生成唯一的主键值。可以采用开源的twitter( 非官方中文惯称：推特.是国外的一个网站，是一个社交网络及微博客服务) 的snowflake （雪花）算法。 默认情况下41bit的时间戳可以支持该算法使用到2082年，10bit的工作机器id可以支持1024台机器，序列号支持1毫秒产生4096个自增序列id . SnowFlake的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162package util;import java.lang.management.ManagementFactory;import java.net.InetAddress;import java.net.NetworkInterface;/** * &lt;p&gt;名称：IdWorker.java&lt;/p&gt; * &lt;p&gt;描述：分布式自增长ID&lt;/p&gt; * &lt;pre&gt; * Twitter的 Snowflake JAVA实现方案 * &lt;/pre&gt; * 核心代码为其IdWorker这个类实现，其原理结构如下，我分别用一个0表示一位，用—分割开部分的作用： * 1||0---0000000000 0000000000 0000000000 0000000000 0 --- 00000 ---00000 ---000000000000 * 在上面的字符串中，第一位为未使用（实际上也可作为long的符号位），接下来的41位为毫秒级时间， * 然后5位datacenter标识位，5位机器ID（并不算标识符，实际是为线程标识）， * 然后12位该毫秒内的当前毫秒内的计数，加起来刚好64位，为一个Long型。 * 这样的好处是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和机器ID作区分）， * 并且效率较高，经测试，snowflake每秒能够产生26万ID左右，完全满足需要。 * &lt;p&gt; * 64位ID (42(毫秒)+5(机器ID)+5(业务编码)+12(重复累加)) * * @author Polim */public class IdWorker &#123; // 时间起始标记点，作为基准，一般取系统的最近时间（一旦确定不能变动） private final static long twepoch = 1288834974657L; // 机器标识位数 private final static long workerIdBits = 5L; // 数据中心标识位数 private final static long datacenterIdBits = 5L; // 机器ID最大值 private final static long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); // 数据中心ID最大值 private final static long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); // 毫秒内自增位 private final static long sequenceBits = 12L; // 机器ID偏左移12位 private final static long workerIdShift = sequenceBits; // 数据中心ID左移17位 private final static long datacenterIdShift = sequenceBits + workerIdBits; // 时间毫秒左移22位 private final static long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private final static long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /* 上次生产id时间戳 */ private static long lastTimestamp = -1L; // 0，并发控制 private long sequence = 0L; private final long workerId; // 数据标识id部分 private final long datacenterId; public IdWorker() &#123; this.datacenterId = getDatacenterId(maxDatacenterId); this.workerId = getMaxWorkerId(datacenterId, maxWorkerId); &#125; /** * @param workerId 工作机器ID * @param datacenterId 序列号 */ public IdWorker(long workerId, long datacenterId) &#123; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;worker Id can&#x27;t be greater than %d or less than 0&quot;, maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;datacenter Id can&#x27;t be greater than %d or less than 0&quot;, maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; /** * 获取下一个ID * * @return */ public synchronized long nextId() &#123; long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException(String.format(&quot;Clock moved backwards. Refusing to generate id for %d milliseconds&quot;, lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; // 当前毫秒内，则+1 sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; // 当前毫秒内计数满了，则等待下一秒 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0L; &#125; lastTimestamp = timestamp; // ID偏移组合生成最终的ID，并返回ID long nextId = ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | (datacenterId &lt;&lt; datacenterIdShift) | (workerId &lt;&lt; workerIdShift) | sequence; return nextId; &#125; private long tilNextMillis(final long lastTimestamp) &#123; long timestamp = this.timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = this.timeGen(); &#125; return timestamp; &#125; private long timeGen() &#123; return System.currentTimeMillis(); &#125; /** * &lt;p&gt; * 获取 maxWorkerId * &lt;/p&gt; */ protected static long getMaxWorkerId(long datacenterId, long maxWorkerId) &#123; StringBuffer mpid = new StringBuffer(); mpid.append(datacenterId); String name = ManagementFactory.getRuntimeMXBean().getName(); if (!name.isEmpty()) &#123; /* * GET jvmPid */ mpid.append(name.split(&quot;@&quot;)[0]); &#125; /* * MAC + PID 的 hashcode 获取16个低位 */ return (mpid.toString().hashCode() &amp; 0xffff) % (maxWorkerId + 1); &#125; /** * &lt;p&gt; * 数据标识id部分 * &lt;/p&gt; */ protected static long getDatacenterId(long maxDatacenterId) &#123; long id = 0L; try &#123; InetAddress ip = InetAddress.getLocalHost(); NetworkInterface network = NetworkInterface.getByInetAddress(ip); if (network == null) &#123; id = 1L; &#125; else &#123; byte[] mac = network.getHardwareAddress(); id = ((0x000000FF &amp; (long) mac[mac.length - 1]) | (0x0000FF00 &amp; (((long) mac[mac.length - 2]) &lt;&lt; 8))) &gt;&gt; 6; id = id % (maxDatacenterId + 1); &#125; &#125; catch (Exception e) &#123; System.out.println(&quot; getDatacenterId: &quot; + e.getMessage()); &#125; return id; &#125;&#125; 在启动类中注册bean 12345678910111213141516171819import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.context.annotation.Bean;import util.IdWorker;@SpringBootApplication//@CrossOrigin //允许跨域@EnableEurekaClientpublic class BaseApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BaseApplication.class); &#125; @Bean public IdWorker idWorker()&#123; return new IdWorker(1, 1); &#125;&#125;","path":"2021/12/27/分布式ID生成器/","date":"12-27","excerpt":"由于我们的数据库在生产环境中要分片部署（MyCat）,所以我们不能使用数据库本身的自增功能来产生主键值，只能由程序来生成唯一的主键值。可以采用开源的twitter( 非官方中文惯称：推特.是国外的一个网站，是一个社交网络及微博客服务) 的snowflake （雪花）算法。","tags":[{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"}]},{"title":"Spring Cloud","text":"Spring Cloud简介 什么是Spring Cloud Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、熔断器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。 Spring Cloud和Spring Boot的关系 Spring Boot 是 Spring 的一套快速配置脚手架，可以基于Spring Boot 快速开发单个微服务，Spring Cloud是一个基于Spring Boot实现的云应用开发工具；Spring Boot专注于快速、方便集成的单个微服务个体，Spring Cloud关注全局的服务治理框架；Spring Boot使用了默认大于配置的理念，很多集成方案已经帮你选择好了，能不配置就不配置，Spring Cloud很大的一部分是基于Spring Boot来实现，可以不基于Spring Boot吗？不可以。 Spring Boot可以离开Spring Cloud独立使用开发项目，但是Spring Cloud离不开Spring Boot，属于依赖的关系。 Spring Cloud主要框架 服务发现–Netflix Eureka 服务调用–Netflix Feign 熔断器–Netflix Hystrix 服务网关–Netflix Zuul 分布式配置–Spring Cloud Config 消息总线–Spring Cloud Bus 服务发现组件 Eureka Eureka Eureka是Netflix开发的服务发现框架，SpringCloud将它集成在自己的子项目spring-cloud-netflix中，实现SpringCloud的服务发现功能。Eureka包含两个组件：Eureka Server和Eureka Client。 Eureka Server提供服务注册服务，各个节点启动后，会在Eureka Server中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观的看到。 Eureka Client是一个java客户端，用于简化与Eureka Server的交互，客户端同时也就别一个内置的、使用轮询(round-robin)负载算法的负载均衡器。在应用启动后，将会向Eureka Server发送心跳,默认周期为30秒，如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，Eureka Server将会从服务注册表中把这个服务节点移除(默认90秒)。 Eureka Server之间通过复制的方式完成数据的同步，Eureka还提供了客户端缓存机制，即使所有的Eureka Server都挂掉，客户端依然可以利用缓存中的信息消费其他服务的API。综上，Eureka通过心跳检查、客户端缓存等机制，确保了系统的高可用性、灵活性和可伸缩性。 Eureka服务端开发 （1）创建hbgydx_eureka模块 （2）引入依赖 父工程pom.xml定义Spring Cloud版本 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.M9&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; hbgydx_eureka模块pom.xml引入eureka-server 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; (3)添加application.yml 123456789server: port: 6868 #服务端口eureka: client: registerWithEureka: false #是否将自己注册到Eureka服务中，本身就是所yi无需注册 fetchRegistry: false #是否从Eureka中获取注册信息 serviceUrl: #Eureka客户端与Eureka服务端进行交互的地址 defaultZone: http://127.0.0.1:$&#123;server.port&#125;/eureka/ （4）编写启动类 12345678@SpringBootApplication@EnableEurekaServerpublic class EurekaServer &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServer.class, args); &#125;&#125; (5)启动运行启动类，然后在浏览器地址栏输入 http://localhost:6868/ 运行.主界面中system status为系统信息 General Info为一般信息 Instances currently registered with Eureka为注册到的所有微服务列表 服务注册 我们现在就将所有的微服务都注册到Eureka中，这样所有的微服务之间都可以互相调用了。（1）将其他微服务模块添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; (2)修改每个微服务的application.yml,添加注册eureka服务配置 123456eureka: client: service-url: defaultZone: http://localhost:6868/eureka instance: prefer-ip-address: true (3)修改每个服务i类的启动类，添加注解：@EnableEurekaClient (4)启动测试：将每个微服务启动起来，会发现eureka的注册列表中可以看到这些微服务了 Feign实现服务间的调用 Feign简介 Feign是简化Java HTTP客户端开发的工具（java-to-httpclient-binder），它的灵感来自于Retrofit、JAXRS-2.0和WebSocket。Feign的初衷是降低统一绑定Denominator到HTTP API的复杂度，不区分是否为restful。 快速体验 （1）在hbgydx_qa模块添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; (2)修改hbgydx_qa模块的启动类，添加注解 123@EnableDiscoveryClient@EnableFeignClients (3)在hbgydx_qa模块创建com.hbgydx.qa.client包，包下创建接口： 123456@FeignClient(&quot;base&quot;)public interface LabelClient &#123; @RequestMapping(value=&quot;/label/&#123;id&#125;&quot;, method = RequestMethod.GET) public Result findById(@PathVariable(&quot;id&quot;) String id);&#125; @FeignClint注解用于指定从哪个服务中调用功能，注意，里面的名称与调用的服务名保持一致，并且不能有下划线@RequestMapping注解用于对被调用的微服务进行地址映射。注意，@PathVariable注解一定要指定参数名称，否则出错（5）修改hbgydx_qa模块的ProblemController 12345678@Autowiredprivate LabelClient labelClient;@RequestMapping(value = &quot;/label/&#123;labelid&#125;&quot;)public Result findLabelById(@PathVariable String labelid)&#123; Result result = labelClient.findById(labelid); return result;&#125;","path":"2021/12/20/Spring-Cloud/","date":"12-20","excerpt":"Spring Cloud简介 什么是Spring Cloud Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、熔断器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。","tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://tai.social/tags/Spring-Cloud/"},{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"}]},{"title":"Dockerfile","text":"什么是Dockerfile Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 对于开发人员：可以为开发团队提供一个完全一致的开发环境 对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了。 对于运维人员：在部署时，可以实现应用的无缝移植 常用命令 FROM image_name:tag 定义了使用哪个基础镜像启动构建流程 MAINTAINER user_name 声明镜像的创建者 ENV key value 设置环境变量 (可以写多条) RUN command 是Dockerfile的核心部分(可以写多条) ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压. ADD source_dir/file dest_dir/file 将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压 WORKDIR path_dir 设置工作目录 EXPOSE port1 prot2 用来指定端口，使容器内的应用可以通过端口和外界交互 CMD argument 在构建容器时使用，会被docker run 后的argument覆盖 ENTRYPOINT argument 和CMD相似，但是并不会被docker run指定的参数覆盖 VOLUME 将本地文件夹或者其他容器的文件挂载到容器中 使用脚本创建镜像 创建目录 1mkdir –p /usr/local/dockerjdk8 下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的/usr/local/dockerjdk8目录 创建文件Dockerfile vi Dockerfile 123456789101112131415 #依赖镜像名称和IDFROM centos:7#指定镜像创建者信息MAINTAINER ITCAST#切换工作目录WORKDIR /usrRUN mkdir /usr/local/java#ADD 是相对路径jar,把java添加到容器中ADD jdk‐8u171‐linux‐x64.tar.gz /usr/local/java/#配置java环境变量ENV JAVA_HOME /usr/local/java/jdk1.8.0_171ENV JRE_HOME $JAVA_HOME/jreENV CLASSPATH$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHENV PATH $JAVA_HOME/bin:$PATH 执行命令构建镜像 1docker build ‐t=&#x27;jdk1.8&#x27; . 查看镜像是否建立完成 12docker images 创建容器 1docker run ‐it ‐‐name=myjdk8 jdk1.8 /bin/bash","path":"2021/12/15/Dockerfile/","date":"12-15","excerpt":"什么是Dockerfile Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 对于开发人员：可以为开发团队提供一个完全一致的开发环境 对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了。 对于运维人员：在部署时，可以实现应用的无缝移植","tags":[{"name":"java","slug":"java","permalink":"http://tai.social/tags/java/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://tai.social/tags/Spring-Cloud/"}]},{"title":"JJWT","text":"JJWT是一个提供端到端的JWT创建和验证的Java库。永远免费和开源(ApacheLicense，版本2.0)，JJWT很容易使用和理解。它被设计成一个以建筑为中心的流畅界面，隐藏了它的大部分复杂性。 JJWT快速入门 token的创建 创建maven工程，引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.6.0&lt;/version&gt;&lt;/dependency 创建类CreateJwtTest，用于生成token 12345678910public class CreateJwtTest &#123; public static void main(String[] args) &#123; JwtBuilder builder= Jwts.builder().setId(&quot;888&quot;) .setSubject(&quot;小白&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256,&quot;hbgydx&quot;); System.out.println( builder.compact() ); &#125; &#125; setIssuedAt用于设置签发时间 signWith用于设置签名秘钥 测试运行 token的解析 我们刚才已经创建了token ，在web应用中这个操作是由服务端进行然后发给客户端，客户端在下次向服务端发送请求时需要携带这个token（这就好像是拿着一张门票一样），那服务端接到这个token 应该解析出token中的信息（例如用户id）,根据这些信息查询数据库返回相应的结果。 创建ParseJwtTest 123456789101112public class ParseJwtTest &#123; public static void main(String[] args) &#123; String token=&quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiO jE1MjM0MTM0NTh9.gq0J-cOM_qCNqU_s-d_IrRytaNenesPmqAIhQpYXHZk&quot;; Claims claims = Jwts.parser().setSigningKey(&quot;hbgydx&quot;).parseClaimsJws(token).getBody(); System.out.println(&quot;id:&quot;+claims.getId()); System.out.println(&quot;subject:&quot;+claims.getSubject()); System.out.println(&quot;IssuedAt:&quot;+claims.getIssuedAt()); &#125;&#125; 试着将token或签名秘钥篡改一下，会发现运行时就会报错，所以解析token也就是验证token token过期校验 有很多时候，我们并不希望签发的token是永久生效的，所以我们可以为token添加一个过期时间。 创建CreateJwtTest2 12345678910111213public class CreateJwtTest2 &#123; public static void main(String[] args) &#123; //为了方便测试，我们将过期时间设置为1分钟 long now = System.currentTimeMillis();//当前时间 long exp = now + 1000*60;//过期时间为1分钟 JwtBuilder builder= Jwts.builder().setId(&quot;888&quot;) .setSubject(&quot;小白&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256,&quot;hbgydx&quot;) .setExpiration(new Date(exp)); System.out.println( builder.compact() ); &#125;&#125; setExpiration 方法用于设置过期时间 当未过期时可以正常读取，当过期时会引发io.jsonwebtoken.ExpiredJwtException异常。 自定义claims 刚才的例子只是存储了id和subject两个信息，如果你想存储更多的信息（例如角色）可以定义自定义claims 创建CreateJwtTest3 12345678910111213141516public class CreateJwtTest3 &#123; public static void main(String[] args) &#123; //为了方便测试，我们将过期时间设置为1分钟 long now = System.currentTimeMillis();//当前时间 long exp = now + 1000*60;//过期时间为1分钟 JwtBuilder builder= Jwts.builder().setId(&quot;888&quot;) .setSubject(&quot;小白&quot;) .setIssuedAt(new Date()) .signWith(SignatureAlgorithm.HS256,&quot;hbgydx&quot;) .setExpiration(new Date(exp)) .claim(&quot;roles&quot;,&quot;admin&quot;) .claim(&quot;logo&quot;,&quot;logo.png&quot;); System.out.println( builder.compact() ); &#125;&#125; 修改ParseJwtTest 1234567891011121314151617181920public class ParseJwtTest &#123; public static void main(String[] args) &#123; String compactJws=&quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJp YXQiOjE1MjM0MTczMjMsImV4cCI6MTUyMzQxNzM4Mywicm9sZXMiOiJhZG1pbiIsImxvZ28iO iJsb2dvLnBuZyJ9.b11p4g4rE94rqFhcfzdJTPCORikqP_1zJ1MP8KihYTQ&quot;; Claims claims = Jwts.parser().setSigningKey(&quot;hbgydx&quot;).parseClaimsJws(compactJws).getBody( ); System.out.println(&quot;id:&quot;+claims.getId()); System.out.println(&quot;subject:&quot;+claims.getSubject()); System.out.println(&quot;roles:&quot;+claims.get(&quot;roles&quot;)); System.out.println(&quot;logo:&quot;+claims.get(&quot;logo&quot;)); SimpleDateFormat sdf=new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); System.out.println(&quot;签发时间:&quot;+sdf.format(claims.getIssuedAt())); System.out.println(&quot;过期时 间:&quot;+sdf.format(claims.getExpiration())); System.out.println(&quot;当前时间:&quot;+sdf.format(new Date()) ); &#125;&#125;","path":"2021/09/11/JJWT/","date":"09-11","excerpt":"JJWT是一个提供端到端的JWT创建和验证的Java库。永远免费和开源(ApacheLicense，版本2.0)，JJWT很容易使用和理解。它被设计成一个以建筑为中心的流畅界面，隐藏了它的大部分复杂性。","tags":[{"name":"JJWT","slug":"JJWT","permalink":"http://tai.social/tags/JJWT/"},{"name":"jwt","slug":"jwt","permalink":"http://tai.social/tags/jwt/"}]},{"title":"IOU & GIOU & DIOU 介绍及其代码实现","text":"参考这篇博客https://blog.csdn.net/leonardohaig/article/details/103394369","path":"2021/09/02/iou/","date":"09-02","excerpt":"","tags":[{"name":"iou","slug":"iou","permalink":"http://tai.social/tags/iou/"}]},{"title":"基于JWT的Token认证机制实现","text":"什么是JWT JSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在用户和服务器之间传递安全可靠的信息。 JWT组成 一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名。 头部（Header） 头部用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个JSON对象。 &#123;&quot;typ&quot;:&quot;JWT&quot;,&quot;alg&quot;:&quot;HS256&quot;&#125; 在头部指明了签名算法是HS256算法。 我们进行BASE64编码http://base64.xpcha.com/，编码后的字符串如下： eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 小知识：Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于2 的6次方等于64，所以每6个比特为一个单元，对应某个可打印字符。三个字节有24 个比特，对应于4个Base64单元，即3个字节需要用4个可打印字符来表示。JDK 中 提供了非常方便的 BASE64Encoder 和 BASE64Decoder，用它们可以非常方便的 完成基于 BASE64 的编码和解码 载荷（playload） 载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分 标准中注册的声明（建议但不强制使用） iss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 公共的声明 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密 私有的声明 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64 是对称解密的，意味着该部分信息可以归类为明文信息。 这个指的就是自定义的claim。比如前面那个结构举例中的admin和name都属于自定的 claim。这些claim跟JWT标准规定的claim区别在于：JWT规定的claim，JWT的接收方在 拿到JWT之后，都知道怎么对这些标准的claim进行验证(还不知道是否能够验证)；而 private claims不会验证，除非明确告诉接收方要对这些claim进行验证以及规则才行。 定义一个payload: &#123;&quot;sub&quot;:&quot;1234567890&quot;,&quot;name&quot;:&quot;John Doe&quot;,&quot;admin&quot;:true&#125; 然后将其进行base64编码，得到Jwt的第二部分。 eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9 签证（signature） jwt的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的) payload (base64后的) secret 这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。 TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6I kpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7Hg Q 注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。","path":"2021/09/01/基于JWT的Token认证机制实现/","date":"09-01","excerpt":"什么是JWT JSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在用户和服务器之间传递安全可靠的信息。","tags":[{"name":"jwt","slug":"jwt","permalink":"http://tai.social/tags/jwt/"},{"name":"token","slug":"token","permalink":"http://tai.social/tags/token/"}]},{"title":"常见的认证机制","text":"HTTP Basic Auth HTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和password，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产环境下被使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP BasicAuth Cookie Auth Cookie认证机制就是为一次请求认证在服务端创建一个Session对象，同时在客户端的浏览器端创建了一个Cookie对象；通过客户端带上来Cookie对象来与服务器端的session对象匹配来实现状态管理的。默认的，当我们关闭浏览器的时候，cookie会被删除。但可以通过修改cookie 的expire time使cookie在一定时间内有效； OAuth OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在某一web服务上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。OAuth允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的第三方系统（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth让用户可以授权第三方网站访问他们存储在另外服务提供者的某些特定信息，而非所有内容 这种基于OAuth的认证机制适用于个人消费者类的互联网产品，如社交类APP等应用，但是不太适合拥有自有认证权限管理的企业应用。 Token Auth 使用基于 Token 的身份验证方法，在服务端不需要存储用户的登录记录。大概的流程是这样的： 客户端使用用户名跟密码请求登录 服务端收到请求，去验证用户名与密码 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据 Token机制相对于Cookie机制又有什么好处呢？ 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提是传输的用户认证信息通过HTTP头传输. 无状态(也称：服务端可扩展行):Token机制在服务端不需要存储session信息，因为Token 自身包含了所有登录用户的信息，只需要在客户端的cookie或本地介质存储状态信息. 更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，HTML,图片等），而你的服务端只要提供API即可. 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在你的API被调用的时候，你可以进行Token生成调用即可. 更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认证机制就会简单得多。 CSRF:因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防范。 性能: 一次网络往返时间（通过数据库查询session信息）总比做一次HMACSHA256计算 的Token验证和解析要费时得多. 不需要为登录页面做特殊处理: 如果你使用Protractor 做功能测试的时候，不再需要为登录页面做特殊处理. 基于标准化:你的API可以采用标准化的 JSON Web Token (JWT). 这个标准已经存在多个后端库（.NET, Ruby, Java,Python, PHP）和多家公司的支持（如：Firebase,Google, Microsoft）.","path":"2021/09/01/常见的认证机制/","date":"09-01","excerpt":"HTTP Basic Auth HTTP Basic Auth简单点说明就是每次请求API时都提供用户的username和password，简言之，Basic Auth是配合RESTful API 使用的最简单的认证方式，只需提供用户名密码即可，但由于有把用户名密码暴露给第三方客户端的风险，在生产环境下被使用的越来越少。因此，在开发对外开放的RESTful API时，尽量避免采用HTTP BasicAuth","tags":[{"name":"java","slug":"java","permalink":"http://tai.social/tags/java/"},{"name":"token","slug":"token","permalink":"http://tai.social/tags/token/"}]},{"title":"密码加密","text":"BCrypt密码加密 Spring Security提供了BCryptPasswordEncoder类,实现Spring的PasswordEncoder接口使用BCrypt强哈希方法来加密密码。 BCrypt强哈希方法 每次加密的结果都不一样。 引入pom依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置类 我们在添加了spring security依赖后，所有的地址都被spring security所控制了，我们目 前只是需要用到BCrypt密码加密的部分，所以我们要添加一个配置类，配置为所有地址 都可以匿名访问 12345678910111213141516/** * 安全配置类 */ @Configuration @EnableWebSecurity public class WebSecurityConfig extends WebSecurityConfigurerAdapter&#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(&quot;/**&quot;).permitAll() .anyRequest().authenticated() .and().csrf().disable(); &#125; &#125; 修改Application, 配置bean 1234@Beanpublic BCryptPasswordEncoder bcryptPasswordEncoder()&#123;return new BCryptPasswordEncoder();&#125; 密码加密 12345678910@AutowiredBCryptPasswordEncoder encoder; public void add(Admin admin) &#123; admin.setId(idWorker.nextId()+&quot;&quot;); //主键值 //密码加密 String newpassword = encoder.encode(admin.getPassword());//加密后 的密码 admin.setPassword(newpassword); adminDao.save(admin);&#125; 密码校验 service添加 1234567891011121314151617/*** 根据登陆名和密码查询* @param loginname* @param password* @return*/public Admin findByLoginnameAndPassword(String loginname, Stringpassword)&#123; Admin admin = adminDao.findByLoginname(loginname); if( admin!=null &amp;&amp; encoder.matches(password,admin.getPassword())) &#123; return admin; &#125;else&#123; return null; &#125;&#125; controller添加 123456789101112131415161718/** * 用户登陆 * @param loginname * @param password * @return */ @RequestMapping(value=&quot;/login&quot;,method=RequestMethod.POST) public Result login(@RequestBody Map&lt;String,String&gt; loginMap)&#123; Admin admin = adminService.findByLoginnameAndPassword(loginMap.get(&quot;loginname&quot;), loginMap.get(&quot;password&quot;)); if(admin!=null)&#123; return new Result(true,StatusCode.OK,&quot;登陆成功&quot;); &#125;else&#123; return new Result(false,StatusCode.LOGINERROR,&quot;用户名或密码错 误&quot;); &#125; &#125;","path":"2021/08/31/密码加密与JWT/","date":"08-31","excerpt":"BCrypt密码加密 Spring Security提供了BCryptPasswordEncoder类,实现Spring的PasswordEncoder接口使用BCrypt强哈希方法来加密密码。","tags":[{"name":"密码加密","slug":"密码加密","permalink":"http://tai.social/tags/%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86/"}]},{"title":"重装系统后精灵标记助手缓存找回","text":"在我一次冲动重装系统后，精灵标记助手之前标记的数据没有导出，导致数据无了。。。但是我又不甘心的找回来了，hhhhhhhhhhhhhhhh 找回方法 找到windows.old,我的路径是这个C:\\Windows.old\\Users\\Tai\\AppData\\Roaming\\Colabeler 然后找到C:\\Users\\Tai\\AppData\\Roaming\\Colabeler，将数据覆盖就好，可以直接删掉复制","path":"2021/08/24/重装系统后精灵标记助手缓存找回/","date":"08-24","excerpt":"在我一次冲动重装系统后，精灵标记助手之前标记的数据没有导出，导致数据无了。。。但是我又不甘心的找回来了，hhhhhhhhhhhhhhhh","tags":[]},{"title":"SSL VPN无法连接","text":"记一次惨痛的教训。SSL VPN 无法连接，获取不到ip资源的问题。 安装上VPN后，虽然可以启动，但是VPN一会自己就掉了，而且无法连接ip地址。 先查看client.log 捕捉到的信息 1234567 [INFOR][2021-08-22 21:44:19 187]Assigned IP: 3.0.1.6:255.255.254.0 [ERROR][2021-08-22 21:44:19 344]TapdevOpen(): Cannot open device \\\\.\\Global\\&#123;8EFE0A78-C826-4099-B0A9-26D78E0A3FDB&#125;.tap: 2 [ERROR][2021-08-22 21:44:19 375]Failed to open TAP device,So sleep a little time and try again! [ERROR][2021-08-22 21:44:20 626]TapdevOpen(): Cannot open device \\\\.\\Global\\&#123;8EFE0A78-C826-4099-B0A9-26D78E0A3FDB&#125;.tap: 2 [ERROR][2021-08-22 21:44:20 648]Failed to open TAP device,So sleep a little time and try again! [ERROR][2021-08-22 21:44:21 869]TapdevOpen(): Cannot open device \\\\.\\Global\\&#123;8EFE0A78-C826-4099-B0A9-26D78E0A3FDB&#125;.tap: 2 [INFOR][2021-08-22 21:44:21 928]Sslvpn client clean all and exit normally! 查看user.log 这么看没问题，但是和正常的user.log对比 没有请求id资源。我只知道有设备没有启动但是不知道是哪个，因为有一个vpn可以运行所以一开始就排除了虚拟网卡，但是问题就是虚拟网卡。 因为解决不了问题，脑子一热就重装了系统，但是没想到我用精灵标注标注的800张图片，竟然缓存在C盘，所以我花了十多天标的数据无了。。。。。。(后续就是，我在windows.old里找回来了) 重装系统后问题还是没有解决。所以我寻找TAP设备，打开设备管理器。 当时TAP是黄色的，显示签名有问题。 所以应该是它的问题。 解决办法是进bios，关掉secure boot.","path":"2021/08/23/SSL-VPN无法连接/","date":"08-23","excerpt":"记一次惨痛的教训。SSL VPN 无法连接，获取不到ip资源的问题。","tags":[{"name":"SSL VPN","slug":"SSL-VPN","permalink":"http://tai.social/tags/SSL-VPN/"}]},{"title":"Java笔试","text":"1.final应用场景 用来修饰数据，包括成员变量和局部变量，该变量只能被赋值一次且它的值无法被改变。对于成员变量来讲，我们必须在声明时或者构造方法中对它赋值； 用来修饰方法参数，表示在变量的生存期中它的值不能被改变； 修饰方法，表示该方法无法被重写； 修饰类，表示该类无法被继承。 2.Redis数据结构 Redis 有 5 种基础数据结构，它们分别是：string(字符串)、list(列表)、hash(字典)、set(集合) 和 zset(有序集合)。 3.hashmap和treemap查找效率 特性对比 hashmap是无序的，treemap是有序的，整个key是按照自然顺序来的。 hashmap可以put一个null当key ，treemap却不支持。 底层结构不一样，一个是数组➕红黑树，一个直接就是红黑树。 TreeMap取出来的是排序后的键值对。插入、删除需要维护平衡会牺牲一些效率。但如果要按自然顺序或自定义顺序遍历键，那么TreeMap会更好。 HashMap比TreeMap的效率要高 4.Redis持久化 Redis所有数据保存在内存中，对数据的更新将异步地保存到磁盘上，使得数据在Redis重启之后仍然存在。 Redis提供了两种不同的持久化方法将数据保存到硬盘里面。 快照持久化：将Redis某一时刻存在的所有数据都写入硬盘。Redis通过创建快照来获得存储在内存里面的数据在某个时间节点上的副本。 AOF持久化：AOF的全称叫append-only file，中文意思是只追加文件。当使用AOF持久化方式的时候，Redis执行写命令的时候，将被执行的写命令复制到硬盘里面，说的通俗一点就是写日志。 AOF持久化将被执行的写命令写到AOF文件的末尾，以达到记录数据的目的。Redis只要从头到尾重新执行一次AOF所有的命令就可以恢复数据。 5.MySQL索引 索引是一个单独的、存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针。使用索引用于快速找出在某个或多个列中有一特定值的行，所有MySQL列类型都可以被索引，对相关列使用索引是提高查询操作速度的最佳途径。 6.原子性 原子性是指一个操作是不可中断的，要么全部执行成功要么全部执行失败，有着“同生共死”的感觉。及时在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程所干扰。 7.引用类型 在 JDK.1.2 之后，Java 对引用的概念进行了扩充，将引用分为了：强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4 种，这 4 种引用的强度依次减弱。 强引用 Java默认的声明就是强引用，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足时，JVM也会直接抛出OutOfMemoryError，不会去回收。如果想中断强引用与对象之间的联系，可以显示的将强引用赋值为null，这样一来，JVM就可以适时的回收对象了 软引用 软引用是用来描述一些非必需但仍有用的对象。在内存足够的时候，软引用对象不会被回收，只有在内存不足时，系统则会回收软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。这种特性常常被用来实现缓存技术，比如网页缓存，图片缓存等。 在 JDK1.2 之后，用java.lang.ref.SoftReference类来表示软引用。 弱引用 弱引用的引用强度比软引用要更弱一些，无论内存是否足够，只要 JVM 开始进行垃圾回收，那些被弱引用关联的对象都会被回收。在 JDK1.2 之后，用 java.lang.ref.WeakReference 来表示弱引用。 虚引用 虚引用是最弱的一种引用关系，如果一个对象仅持有虚引用，那么它就和没有任何引用一样，它随时可能会被回收，在 JDK1.2 之后，用 PhantomReference 类来表示，通过查看这个类的源码，发现它只有一个构造函数和一个 get() 方法，而且它的 get() 方法仅仅是返回一个null，也就是说将永远无法通过虚引用来获取对象，虚引用必须要和 ReferenceQueue 引用队列一起使用。","path":"2021/08/11/Java笔试/","date":"08-11","excerpt":"1.final应用场景 用来修饰数据，包括成员变量和局部变量，该变量只能被赋值一次且它的值无法被改变。对于成员变量来讲，我们必须在声明时或者构造方法中对它赋值； 用来修饰方法参数，表示在变量的生存期中它的值不能被改变； 修饰方法，表示该方法无法被重写； 修饰类，表示该类无法被继承。","tags":[{"name":"Java笔试","slug":"Java笔试","permalink":"http://tai.social/tags/Java%E7%AC%94%E8%AF%95/"}]},{"title":"Nginx用途","text":"一、静态代理 二、负载均衡 三、限流 四、缓存 五、黑白名单","path":"2021/08/10/Nginx用途/","date":"08-10","excerpt":"","tags":[{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"},{"name":"Nginx","slug":"Nginx","permalink":"http://tai.social/tags/Nginx/"}]},{"title":"RS Loss","text":"本文作者提出了一种用于目标检测和实例分割任务的Rank &amp; Sort Los），可以用来涨点。 在多阶段的目标检测器上，RS Loss在标准的Faster R-CNN上达到39.6 AP，并且超过了FPN 3.4AP，aLRP 2.2AP，Dynamic R-CNN 0.7AP。 与Focal Loss比较,当两个网络在没有辅助head的情况下训练时，RS Loss提供大约 1 AP的增益。 论文地址：https://arxiv.org/abs/2107.11669 代码地址：https://github.com/kemaloksuz/RankSortLoss RS Loss其实由两部分组成，一部分是Rank，还有一部分是Sort。Rank指的是根据Classification Lofits区分出正负的样本，Sort指的是根据IoU来对正样本进行排序。 好处 因为正样本根据IoU进行Sort，不同的正样本在训练的时候就会有不同的优先级，所以用RS Loss训练的检测器就不再需要额外的辅助head 了。 由于RS Loss根据Classification Lofits区分出正负的样本，因此用RS Loss训练的检测器就能够处理比较极端的不平衡数据分布 ，而不需要采用启发式的采样。 此外，除了学习率，RS Loss不需要任何超参数调优 ，因为在RS Loss中没有需要调优的任务平衡系数。 损失函数定义 直接衡量了目标和期望误差之间的差异，产生一个可解释的损失值，解决了D1的问题； 不需要限制i是数据集中的正样本，这样的定义更加完整。 损失函数计算 损失函数优化 在原始的检测任务中，损失函数通常由三部分组成： 其中cls为 Focal Loss，box为 GIoU Loss ，str为 Cross-entropy Loss。 在本文中作者首先砍掉了centerness head，然后 RS Loss代替了，得到： 比较图 在上图中，(a)是一个以前的Visual Detector的流程，包括可能来自多个阶段的多个子任务的head；（b）是使用RS Loss进行训练的Visual Detector；（c）展示了如何通过loss的值来平衡每个任务的权重，从而避免了调参。","path":"2021/08/09/RS-Loss/","date":"08-09","excerpt":"本文作者提出了一种用于目标检测和实例分割任务的Rank &amp; Sort Los），可以用来涨点。 在多阶段的目标检测器上，RS Loss在标准的Faster R-CNN上达到39.6 AP，并且超过了FPN 3.4AP，aLRP 2.2AP，Dynamic R-CNN 0.7AP。 与Focal Loss比较,当两个网络在没有辅助head的情况下训练时，RS Loss提供大约 1 AP的增益。","tags":[{"name":"Loss","slug":"Loss","permalink":"http://tai.social/tags/Loss/"}]},{"title":"RequestBody和RequestParam","text":"RequestParam接收的是Json对象，RequestBody接收的是Json字符串 通常Ajax传的是Json对象，所以后台可以使用RequestParam Map&lt;String,String&gt; map接收，如果想使用RequestBody Map&lt;String,String&gt; map ，要使用JSON.stringify(data)的方式就能将对象变成字符串。同时ajax请求的时候也要指定dataType: “json”,contentType:”application/json” 这样就可以轻易的将一个对象或者List传到Java端，使用@RequestBody即可绑定对象或者List.","path":"2021/08/04/RequestBody和RequestParam/","date":"08-04","excerpt":"","tags":[{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"}]},{"title":"CANet中出现ValueError: could not broadcast input array from shape (x,5) into shape (100,5)","text":"当用CANet运行自己的数据集的时候会出现ValueError: could not broadcast input array from shape (x,5) into shape (100,5)原因是标签数量问题，修改configure.py 文件中的MAX_NUM_GT_BOXES为你最大的标签数量","path":"2021/07/20/CANet中出现ValueError-could-not-broadcast-input-array-from-shape-x-5-into-shape-100-5/","date":"07-20","excerpt":"","tags":[{"name":"CANet","slug":"CANet","permalink":"http://tai.social/tags/CANet/"}]},{"title":"ElasticSearch","text":"Elasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 ElasticSearch体系结构 ElasticSearch部署 下载ElasticSearch 5.6.8版本https://www.elastic.co/downloads/past-releases/elasticsearch-5-6-8 无需安装，解压安装包后即可使用 在命令提示符下，进入ElasticSearch安装目录下的bin目录,执行命令 1elasticsearch 即可启动。 我们打开浏览器，在地址栏输入http://127.0.0.1:9200/ 即可看到输出结果 IK分词器 什么是IK分词器 我们在浏览器地址栏输入http://127.0.0.1:9200/_analyze?analyzer=chinese&amp;pretty=true&amp;text=我是程序员，浏览器显示效果如下 12345678910111213141516171819202122232425262728293031323334353637383940&#123;&quot;tokens&quot; : [&#123;&quot;token&quot; : &quot;我&quot;,&quot;start_offset&quot; : 0,&quot;end_offset&quot; : 1,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 0&#125;,&#123;&quot;token&quot; : &quot;是&quot;,&quot;start_offset&quot; : 1,&quot;end_offset&quot; : 2,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 1&#125;,&#123;&quot;token&quot; : &quot;程&quot;,&quot;start_offset&quot; : 2,&quot;end_offset&quot; : 3,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 2&#125;,&#123;&quot;token&quot; : &quot;序&quot;,&quot;start_offset&quot; : 3,&quot;end_offset&quot; : 4,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 3&#125;,&#123;&quot;token&quot; : &quot;员&quot;,&quot;start_offset&quot; : 4,&quot;end_offset&quot; : 5,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 4&#125;]&#125; 默认的中文分词是将每个字看成一个词，这显然是不符合要求的，所以我们需要安装中文分词器来解决这个问题。 IK分词是一款国人开发的相对简单的中文分词器。虽然开发者自2012年之后就不在维护了，但在工程应用中IK算是比较流行的一款！ IK分词器安装 下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases 下载5.6.8版本 将ik文件夹拷贝到elasticsearch/plugins 目录下。 重新启动，即可加载IK分词器 IK提供了两个分词算法ik_smart 和 ik_max_word其中 ik_smart 为最少切分，ik_max_word为最细粒度划分 自定义词库 进入elasticsearch/plugins/ik/config目录 新建一个my.dic文件，编辑内容： 1河北工业大学 修改IKAnalyzer.cfg.xml（在ik/config目录下） 1234567&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!‐‐用户可以在这里配置自己的扩展字典 ‐‐&gt; &lt;entry key=&quot;ext_dict&quot;&gt;my.dic&lt;/entry&gt; &lt;!‐‐用户可以在这里配置自己的扩展停止词字典‐‐&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;&lt;/entry&gt;&lt;/properties&gt; 重新启动elasticsearch","path":"2021/07/18/ElasticSearch/","date":"07-18","excerpt":"Elasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。","tags":[{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://tai.social/tags/ElasticSearch/"}]},{"title":"MongoDB","text":"MongoDB 是一个跨平台的，面向文档的数据库，是当前 NoSQL 数据库产品中最热门的一种。它介于关系数据库和非关系数据库之间，是非关系数据库当中功能最丰富，最像关系数据库的产品。它支持的数据结构非常松散，是类似 JSON 的 BSON 格式，因此可以存储比较复杂的数据类型。主要用于吐槽，评论等数据的保存主要是用于 数据流量大而且可以丢失少量数据 引入依赖 12345678&lt;dependencies&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver&lt;/artifactId&gt; &lt;version&gt;3.6.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建application.yml 12345678910server: port: 9006spring: application: name: tai-spit #指定服务名 data: mongodb: host: 192.168.9.2 database: spitdb 创建启动类 1234567@SpringBootApplicationpublic class SpitApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpitApplication.class, args); &#125;&#125; 再创建自己的实体类 创建Dao 123public interface SpitDao extends MongoRepository&lt;Spit, String&gt;&#123;&#125; 再创建service层，在service里面使用springdata的增删改查就可以了","path":"2021/07/15/MongoDB/","date":"07-15","excerpt":"MongoDB 是一个跨平台的，面向文档的数据库，是当前 NoSQL 数据库产品中最热门的一种。它介于关系数据库和非关系数据库之间，是非关系数据库当中功能最丰富，最像关系数据库的产品。它支持的数据结构非常松散，是类似 JSON 的 BSON 格式，因此可以存储比较复杂的数据类型。主要用于吐槽，评论等数据的保存主要是用于 数据流量大而且可以丢失少量数据","tags":[{"name":"java","slug":"java","permalink":"http://tai.social/tags/java/"},{"name":"springDataMongoDB","slug":"springDataMongoDB","permalink":"http://tai.social/tags/springDataMongoDB/"}]},{"title":"SpringDataRedis","text":"最近做springcloud项目，接触到了springDataRedis Spring-data-redis是spring大家族的一部分，提供了在srping应用中通过简单的配置访问redis服务，对reids底层开发包(Jedis, JRedis, and RJC)进行了高度封装，RedisTemplate提供了redis各种操作。 首先导入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 修改application.yml ,在spring节点下添加配置 redis: host: 192.168.9.2 修改ArticleService 引入RedisTemplate，并修改findById方法 12345678910111213141516171819@Autowiredprivate RedisTemplate redisTemplate;/*** 根据ID查询实体* @param id* @return*/public Article findById(String id) &#123; //从缓存中提取 Article article= (Article)redisTemplate.opsForValue().get(&quot;article_&quot;+id); // 如果缓存没有则到数据库查询并放入缓存 if(article==null) &#123; article = articleDao.findById(id).get(); redisTemplate.opsForValue().set(&quot;article_&quot; + id, article); &#125; return article;&#125; 这样在查询的时候，就会自动将文章放入缓存 修改或删除后清除缓存 12345678910111213141516/*** 修改* @param article*/public void update(Article article) &#123; redisTemplate.delete( &quot;article_&quot; + article.getId() );//删除缓存 articleDao.save(article);&#125;/*** 删除* @param id*/public void deleteById(String id) &#123; redisTemplate.delete( &quot;article_&quot; + id );//删除缓存 articleDao.deleteById(id);&#125; 缓存过期处理 1redisTemplate.opsForValue().set(&quot;article_&quot; + id, article,1,TimeUnit.DAYS);","path":"2021/07/15/SpringDataRedis/","date":"07-15","excerpt":"最近做springcloud项目，接触到了springDataRedis","tags":[{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"},{"name":"springDataRedis","slug":"springDataRedis","permalink":"http://tai.social/tags/springDataRedis/"}]},{"title":"Distilling Object Detectors via Decoupled Features","text":"本次工作作者提出一种简单而高效的通过解耦特征进行目标检测的蒸馏方法。通过分析并证明了背景区域在蒸馏过程中的重要作用。后引入 DeFeat 方法，在该方法中，特征在 FPN 水平和 RoI-aligned 特征水平上被分割成物体和背景部分，并对这两部分分别进行蒸馏处理。 DeFeat 具有很强的泛化能力，可以很容易地用于one-stage 和 two-stage 的检测框架。大量实验也验证了 DeFeat 的有效性，并且超越了最先进的目标检测的蒸馏方法。例如，DeFeat 将基于 ResNet50 的 Faster R-CNN的 mAP 从 37.4% 提高到 40.9%，并将基于 ResNet50 的RetinaNet 在COCO 基准上的 mAP 从 36.5% 提高到 39.7%。 作者 | Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu, Chang Xu Abstract 知识提炼是从复杂的教师网络继承信息到紧凑的学生网络并保持强大性能的一种广泛使用的范式。与图像分类不同，目标检测器更加复杂，具有多个损失函数，其中语义信息所依赖的特征错综复杂。在本文中，我们指出从排除对象的区域中得到的特征信息对于提取学生检测器也是必不可少的，这在现有的方法中通常被忽略。此外，我们阐明了在蒸馏过程中，来自不同区域的特征应被赋予不同的重要性。为此，我们提出了一种新的蒸馏算法，该算法通过解耦特征(FAuLT)来学习更好的学习检测器。具体来说，将处理两个层次的解耦特征，以便将有用信息嵌入到学生中，即，从颈部解耦的特征和从分类头解耦的建议。在具有不同主干的各种检测器上的大量实验表明，所提出的able能够超越最先进的蒸馏方法用于物体检测 1. Introduction 作为计算机视觉的基本任务之一，目标检测在包括自动驾驶和监控视频分析在内的各种现实应用中受到越来越多的关注。深度学习的最新进展引入了许多基于卷积神经网络的目标检测解决方案。检测器的主干通常由大量卷积运算组成，以产生对检测至关重要的密集特征准确性。但是这样做不可避免地会导致计算资源成本的急剧增加和检测速度的明显下降。诸如量化[19，58，31，57，62]、剪枝[2，17，20]、网络设计[55，49，15，18]和知识提炼[56，6]等技术已经被开发来克服这种困境并实现对检测任务的有效推断。我们对知识提炼特别感兴趣[24]，因为它提供了一种优雅的方式来学习一个紧凑的学生网络，当性能证明的教师网络可用时。经典的知识提取方法首先被开发用于分类任务，以决定图像属于哪个类别。来自优化良好的教师网络的软标签输出[24，28，38，13]或中间特征[1，23，66]的信息已经被很好地利用来学习学生网络，但是这些方法不能直接扩展到需要进一步找出对象在哪里的检测任务。在目标检测任务中有一些研究知识提取的尝试。例如，FGFI [56]要求学生网络在近物体锚位置上模仿教师网络。TADF [47]通过颈部特征中的高斯掩蔽目标区域和检测头中的阳性样本提取学生。这些工作仅从目标区域提取知识，因为背景区域在检测任务中不被认为是感兴趣的。直观地说，在蒸馏过程中，背景区域可能会引入大量噪声，并且很少被探索。但是在进行蒸馏时缺乏对背景区域的全面分析。因此，抛弃背景区域的草率决定可能并不明智。最重要的是，背景信息已经被证明有助于视觉识别[53，46，9，16]。与其猜测背景区域对蒸馏无用甚至有害，不如对背景进行公平彻底的分析，让事实说话.我们首先通过比较两种方法来检验对象和背景区域在知识提取中的作用:(1)仅通过对象区域FPN特征提取和(2)仅通过背景区域FPN特征提取。当通过来自教师检测器的背景区域提取时，学生不会被显著增强，这是理所当然的，因为背景的信息量和噪声较小[56]。然而，在各种模型和数据集上进行大量实验后，我们观察到一个令人惊讶的结果，即仅通过背景区域特征提取学生也可以显著增强学生，甚至获得与通过对象区域提取相似的结果(图4)。我们通过提取背景特征进一步探索性能提升的来源。以COCO的两个类为例(见图1)，我们进行了误差分析[25]，发现通过背景区域的蒸馏有效地减少了背景误报的数量。上述证据表明，背景区域实际上可以作为对目标区域提取的补充。除此之外，先前的文献已经表明，物体和背景之间有很强的关系[53，69]。目标可能性[53]可以写成(Vo和Vb分别是对象区域和背景的特征）所有的概率都与背景信息有关，背景信息提供了找到一个物体的可能性的估计(例如，一个人不可能在房间里找到一辆汽车)。基于背景的先验在不同的图像中有所不同[69]，因此我们需要学习背景特征以进行更好的预测。然而，上述有希望的期望未能被以前的工作证明是正确的[6，30]，这些工作同时考虑了目标和背景区域。虽然他们利用了这两种类型的区域，但与那些只使用对象区域的学生相比，学生并没有显著提高，这似乎与[56]所指出的现象一致。无论是目标区域还是背景区域都可以通过蒸馏独立地有利于目标检测，但是一旦它们被集成在一起，性能会意外下降。原因可能是他们的方法直接集成了这两种类型的区域。从梯度的角度来看，我们在图2中说明了对象和背景区域之间的不一致性。左列图像是从COCO训练集中随机选取的，右列图像是它们在学生检测器中对应的颈部特征梯度。我们可以观察到，来自对象区域的梯度幅度始终大于来自背景区域的梯度幅度。因此，这提醒我们在蒸馏过程中目标区域和背景区域的不同重要性。 基于这些深刻的观察，我们建议分离用于知识提炼的特征，并强调它们在提炼过程中的独特重要性。包括两个级别的功能，即FPN功能和RoI对齐功能。使用地面真值遮罩将FPN特征分割为对象和背景部分，并在教师和学生之间应用均方误差损失。与RoI对齐的特征也使用教师的预测区域建议解耦为正部分和负部分。基于这些解耦的RoI对齐特征生成的分类逻辑是使用KL散度损失提取的。由此产生的BulT算法可以自适应地结合到一级和两级检测器中，以提高检测精度。 为了验证我们的方法，我们在各种场景下对fast R-CNN[43]和retianet[34]进行了广泛的实验，包括在两个常见的检测基准PASCAL VOC [12]和COCO [35]上对浅学生和窄学生进行蒸馏。特别是，在COCO基准测试中，我们的failure将基于ResNet50的FPN从37.4%提高到40.9% mAP，将基于ResNet50的RetinaNet从36.5%提高到39.7% mAP。 2. Related Work Object detection 被认为是最具挑战性的视觉任务之一，该任务旨在发现当给定图像时物体在什么地方。在过去几年中，单级[42，36，34，29，11，67]和双级[43，21，33，26，27，5]探测器的精度都有了显著提高。尽管安装了非常深的主干[59，48]的检测器具有更好的检测精度，但它们在计算成本方面很昂贵，并且很难部署到移动设备上。有一个有趣的研究方向是通过权重量化来压缩大型检测模型[31，57]，用更少的比特来表示参数权重。修剪[37，14，60，52，51]是另一个研究方向，它从一个大的预训练模型中移除不重要的连接来压缩检测器。设计与轻量级主干网络[55，41，45，32，61，64]相结合的检测器也是检测速度更快的趋势。此外，还有一个研究方向是将知识从一个大检测器转移到一个小检测器[6，56，30]，在这个方向上，人们可以在不设计新架构的情况下提升小检测器的性能。 Knowledge distillation(KD)已经成为将大模型压缩成更小更快模型的最有效技术之一。KD最早由Buciluˇ a等人[4]提出，由Hinton等人[24]推广，通过软输出将教师网络的暗知识传递给学生网络。FitNets [44]表明，激活[23]和中间层的特征[39]也可以被视为指导学生网络的知识。此后，KD被广泛应用于分类任务[22，63，3，54，7，65，10]。最近，有几项工作提出用知识蒸馏压缩目标检测器。Chen等人[6]通过所有组件(即颈部特征、分类头和回归头)提取学生，但是整个特征图的模仿和分类头中的提取都忽略了前景和背景的不平衡，这可能导致次优的结果。唐等[50]提出了一级检测器的自适应蒸馏损失，以放大硬样品的损失。Li等人[30]从区域提议中提取了采样特征，但是，仅模仿上述区域可能会导致误导，因为提议有时表现不佳。王等人[56]打算从前景对象区域提取具有细粒度特征的学生。然而，我们发现剩余的背景特征对于提取更好的学生检测器也是至关重要的。 总之，当前的目标检测的蒸馏框架忽略了背景区域在中间特征中的重要作用和分类头中的负区域建议。在这项工作中，我们发现FPN特征中的目标区域和背景区域对于蒸馏都是实用的，并且平等地对待正面和负面建议会抑制检测器更强的性能。因此，我们首先生成一个二进制掩码来分离中间特征，然后提取相应的特征。同时，我们将分类头中的正负建议解耦，进一步提高泛化能力。 3. Distillation via Decoupled Features 通常，对象检测器由三个或四个组件组成:(a)用于提取语义特征的主干；融合多层次特征的瓶颈；(c)用于生成建议书的远程过程网络(仅在两级探测器中)；以及(d)用于对象分类和包围盒回归的头部。提炼的目的是向学生灌输教师的黑暗知识，这些知识可以是中间层的特征，也可以是分类头中区域建议的软预测。将S ∈RH×W×Cand T ∈RH×W×C分别定义为学生和教师的中间特征。经由中间特征的蒸馏可以被表述为: 其中N = HW C为元素总数，γ用于控制蒸馏损失的尺度，φ表示适配层[6]，I表示仿掩模，即[47]中的高斯掩模和[56]中的细粒度掩模。在以前的工作中，只考虑对象区域或均匀提取整个特征图。均匀处理所有区域的方法[6，30]中的蒙版可以看作是全一张量。 给定从RPN输出的K区域建议，分类负责人需要计算所有建议的软标签。通过软预测的蒸馏可以表述为: 其中超参数λ用于平衡不同的损失项，lce和lklde分别记下交叉熵损失和KL散度损失。Yiis第一个提议的基本事实标签，以及学生和教师探测器分别是ys和yt i。学生的总体培训目标可以表述为: 其中，Lregis表示探测器头中的边界框回归损失，RPN表示两级探测器中的RPN损失。 3.1. Decouple Intermediate Features in Distillation 以前的作品要么选择部分区域，要么使用所有区域，但平等对待中间要素上的每个位置。特别是，FGFI [56]假定背景区域会引入大量噪声，并会损害性能。然而，这种直觉判断与我们在实验中观察到的并不一致，如图1所示。经由仅背景区域的蒸馏仍然获得与经由仅目标区域的蒸馏相当的结果。我们得出结论，中间特征中的背景区域可以补充目标区域，进一步帮助学生检测器的训练，但剩下的问题是如何在蒸馏中适当地整合这两种类型的区域。 基于以上观察，我们建议通过解耦特征提取学生。给定大小为H × W的中间特征，我们首先根据地面真值框B生成一个二进制掩码M:. 其中M∑{ 0，1}H×W，如果属于某个对象，则位置(I，j)的值为1，否则为0。具体来说，如果检测器包含可以输出多级特征的特征金字塔网络(FPN)，我们将为每个地面真值框分配其对应的级别，并相应地为每个级别生成掩码M。然后，我们使用生成的二进制掩码来分离颈部特征，如图3所示。中间特征蒸馏公式如下: 3.2. Decouple Region Proposals in Distillation 通过软预测进行知识提取已经广泛应用于分类任务，并且可以用于检测任务中分类头的提取。然而，不同于训练时没有背景类别的分类任务(如CIFAR和ImageNet)，检测头中的对象和背景类别可以有极其不同数量的建议。我们进行实验来探索如图6所示的目标(正)提议和背景(负)提议的分离蒸馏损失。积极提案的提炼损失始终大于消极提案。如果它们没有得到适当的平衡，背景提议产生的小梯度会淹没在正梯度产生的大梯度中，从而限制了进一步的细化。此外，表5显示，与仅使用否定建议相比，平等对待所有建议会得到更差的结果。因此，我们建议在提取分类头时，将区域建议解耦为正建议和负建议，以实现最优收敛。我们将教师检测器生成的区域建议输入到教师和学生的头部，生成如图3所示的类别预测标准。在我们的方法中，正面建议和负面建议是分开处理的。考虑到积极建议的逻辑，我们通过给老师和学生一个温度来软化预测，如下所示: 其中θs和θt分别代表学生和老师的参数。Y = {1，2，…，C}是检测基准的类别。对于属于背景区域的建议，我们通过类似于上述等式的教师和学生的温度Tbgfor来软化预测。为了从老师的检测器中提取学生的知识，我们使用了如下公式: 通过解耦特征(FAuLT)框架提出的蒸馏概述。我们将中间FPN特征中的区域和来自RPN的区域建议解耦，以提取学生检测器。术语“分数(pos)”和“分数(neg)”分别表示正面和负面建议的分类分数。 4. Experiments 4.1. Datasets and Metrics COCO [35]是一个具有挑战性的对象检测基准，它包含80个对象类。我们的训练集是80k个训练图像和35k个验证图像子集(trainval35k)的并集，而验证集是剩余的5k个验证图像(minival)。我们将平均精度视为评估指标，即mAP、AP50、AP75、APS、APMand和APL。最后三个测量不同比例的物体的性能 Pascal VOC [12]包含20个对象类。我们的训练集是VOC 2007 trainval (5K)和VOC 2012 trainval (11K)的并集，验证集是VOC 2007测试(4.9K)。我们使用0.5的IoU报告mAP分数。 4.2. Implementation Details 所有实验均在8个TeslaV100 GPUs上进行。我们的实现基于带有Pytorch框架[40]的mmdetection [8]。We use SGD optimizer with a batch size of 4 images per GPU, all models are trained for 12epochs, known as 1× schedule. 调整输入图像的大小，使其短边在COCO上有800个像素，在VOC上有600个像素。FPN [33]和RetinaNet[34]的初始学习率分别设置为0.04和0.02。And the learning rate is divided by 10 at the 8-th and 11-th epochs.We set momentum as 0.9 and weight decay as 0.0001. 4.3. Main Results 我们首先在COCO [35]基准上验证我们提出的典型两阶段检测框架FPN [33]上的Table的有效性，如表1所示。位于FPN的ResNet50被选为学生检测器，位于FPN的ResNet152被选为教师检测器。“全颈”表示学生是通过平等对待FPN所有地区的特征而被蒸馏出来的，如等式1所示。“解耦”是指通过解耦的FPN特征提取学生，如等式5所示。“全cls”表示所有地区提案在提炼过程中得到同等对待。“解耦-cls”表示区域建议被解耦为正(对象)和负(背景)，如等式8所示。“主干”是指主干特征也被提炼出来。直接提取FPN所有区域的特征达到40.1%的mAP，解耦后的FPN特征可以进一步提高学生检测器0.3%的mAP。通过解耦的FPN特征和RPN建议提取的学生获得了更高的结果，并且我们的解耦建议可以将结果从40.5%提升到40.8% mAP。在蒸馏中进一步采用主干特性将在COCO基准上实现40.9%的mAP，与学生基线模型相比带来3.5%的收益。此外，我们还在典型的一级检测框架retainet上进行了实验[34]，我们的resnet 152-R50-retainet在COCO上将基线对应的mAP从36.5%提高到39.7%。这些结果清楚地阐明了我们提出的一级和二级探测器的多功能性和通用性。 4.4往后没啥可翻译的 5. Conclusion 在这篇文章中，我们提出了一个简单而有效的方法，通过解耦特征提取目标检测。我们分析和论证了背景区域在蒸馏过程中的重要作用。在大量观察的基础上，我们引入了feature方法，该方法在FPN级和RoI对齐特征级将特征分解为目标和背景部分，并分别对这两部分进行提取。failure是通用的，可以很容易地用于一阶段和两阶段检测框架。大量的实验通过始终优于其他蒸馏技术验证了FAuLT的有效性。","path":"2021/07/14/Distilling-Object-Detectors-via-Decoupled-Features/","date":"07-14","excerpt":"本次工作作者提出一种简单而高效的通过解耦特征进行目标检测的蒸馏方法。通过分析并证明了背景区域在蒸馏过程中的重要作用。后引入 DeFeat 方法，在该方法中，特征在 FPN 水平和 RoI-aligned 特征水平上被分割成物体和背景部分，并对这两部分分别进行蒸馏处理。 DeFeat 具有很强的泛化能力，可以很容易地用于one-stage 和 two-stage 的检测框架。大量实验也验证了 DeFeat 的有效性，并且超越了最先进的目标检测的蒸馏方法。例如，DeFeat 将基于 ResNet50 的 Faster R-CNN的 mAP 从 37.4% 提高到 40.9%，并将基于 ResNet50 的RetinaNet 在COCO 基准上的 mAP 从 36.5% 提高到 39.7%。 作者 | Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu, Chang Xu","tags":[{"name":"目标检测","slug":"目标检测","permalink":"http://tai.social/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"数组中只出现一次的数","text":"描述 给定一个整型数组 arr 和一个整数 k(k&gt;1)。已知 arr中只有 1 个数出现一次，其他的数都出现 k次。请返回只出现了 1 次的数。 代码 12345678910111213141516171819public int foundOnceNumber (int[] arr, int k) &#123; // write code here int[] binarySum = new int[32]; for(int i = 0;i &lt; 32;i++)&#123; int sum = 0; for(int num : arr)&#123; sum += (num &gt;&gt; i &amp; 1); &#125; binarySum[i] = sum; &#125; int res = 0; for(int i = 0;i&lt;32;i++)&#123; if(binarySum[i] % k != 0)&#123; res += 1 &lt;&lt; i; &#125; &#125; return res; &#125;","path":"2021/07/13/数组中只出现一次的数/","date":"07-13","excerpt":"描述 给定一个整型数组 arr 和一个整数 k(k&gt;1)。已知 arr中只有 1 个数出现一次，其他的数都出现 k次。请返回只出现了 1 次的数。","tags":[{"name":"java","slug":"java","permalink":"http://tai.social/tags/java/"}]},{"title":"测试","text":"测试文档","path":"2021/07/12/测试/","date":"07-12","excerpt":"","tags":[{"name":"测试","slug":"测试","permalink":"http://tai.social/tags/%E6%B5%8B%E8%AF%95/"}]}],"categories":[],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://tai.social/tags/Spring-Cloud/"},{"name":"缓存","slug":"缓存","permalink":"http://tai.social/tags/%E7%BC%93%E5%AD%98/"},{"name":"Java","slug":"Java","permalink":"http://tai.social/tags/Java/"},{"name":"java","slug":"java","permalink":"http://tai.social/tags/java/"},{"name":"JJWT","slug":"JJWT","permalink":"http://tai.social/tags/JJWT/"},{"name":"jwt","slug":"jwt","permalink":"http://tai.social/tags/jwt/"},{"name":"iou","slug":"iou","permalink":"http://tai.social/tags/iou/"},{"name":"token","slug":"token","permalink":"http://tai.social/tags/token/"},{"name":"密码加密","slug":"密码加密","permalink":"http://tai.social/tags/%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86/"},{"name":"SSL VPN","slug":"SSL-VPN","permalink":"http://tai.social/tags/SSL-VPN/"},{"name":"Java笔试","slug":"Java笔试","permalink":"http://tai.social/tags/Java%E7%AC%94%E8%AF%95/"},{"name":"Nginx","slug":"Nginx","permalink":"http://tai.social/tags/Nginx/"},{"name":"Loss","slug":"Loss","permalink":"http://tai.social/tags/Loss/"},{"name":"CANet","slug":"CANet","permalink":"http://tai.social/tags/CANet/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://tai.social/tags/ElasticSearch/"},{"name":"springDataMongoDB","slug":"springDataMongoDB","permalink":"http://tai.social/tags/springDataMongoDB/"},{"name":"springDataRedis","slug":"springDataRedis","permalink":"http://tai.social/tags/springDataRedis/"},{"name":"目标检测","slug":"目标检测","permalink":"http://tai.social/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"测试","slug":"测试","permalink":"http://tai.social/tags/%E6%B5%8B%E8%AF%95/"}]}